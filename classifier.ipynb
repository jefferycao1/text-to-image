{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the model to train the final classifier\n",
    "\n",
    "The data is in the flower_data folder\n",
    "We manually changed the files to the new flower data in train and test\n",
    "\n",
    "Only 5 test data for class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('flower_data/flower_data/cat_to_name.json', 'r') as f:\n",
    "    cat_to_name = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms, models, utils\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import PIL\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _init_fn(worker_id):\n",
    "   pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 12\n",
    "MEDIUM_SIZE = 30\n",
    "BIGGER_SIZE = 36\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "cwd = os.getcwd()\n",
    "img_dir = os.path.join(cwd, '102flowers')\n",
    "class1_dir = os.path.join(cwd, 'class00001images')\n",
    "data_dir = os.path.join(cwd, 'flower_data', 'flower_data')\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "valid_dir = os.path.join(data_dir, 'valid')\n",
    "test_dir = os.path.join(data_dir, 'test')\n",
    "\n",
    "\n",
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths. Extends\n",
    "    torchvision.datasets.ImageFolder\n",
    "    \"\"\"\n",
    "\n",
    "    # override the __getitem__ method. this is the method dataloader calls\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = self.imgs[index][0]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "        return tuple_with_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABPMAAAFNCAYAAABlmCoMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17d945037f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "class_sample_counts = {}\n",
    "\n",
    "for c in range(1, 103):\n",
    "    l = class_sample_counts.get(c, 0)\n",
    "    class_sample_counts[c] = l + len(os.listdir(test_dir + '/' + str(c)))\n",
    "\n",
    "# Plot showing the class imbalance\n",
    "plt.figure(figsize=(22, 5))    \n",
    "plt.bar(range(len(class_sample_counts)), list(class_sample_counts.values()), align='center')\n",
    "plt.xticks(range(len(class_sample_counts)), list(class_sample_counts.keys()))\n",
    "plt.xticks(rotation=80)\n",
    "plt.title('Samples per Class')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat_to_name.json', 'test', 'train', 'train_1', 'valid']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir('flower_data/flower_data/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "data_transforms = {'train': transforms.Compose([transforms.Resize(256, interpolation=PIL.Image.BILINEAR),\n",
    "                                                transforms.CenterCrop(224),\n",
    "                                                transforms.ToTensor(),\n",
    "                                               ]),\n",
    "                   'valid': transforms.Compose([transforms.Resize(256, interpolation=PIL.Image.BILINEAR),\n",
    "                                                transforms.CenterCrop(224),\n",
    "                                                transforms.ToTensor(),\n",
    "                                               ])}\n",
    "                   \n",
    "image_datasets = {'train': ImageFolderWithPaths(train_dir, transform=data_transforms['train']),\n",
    "                  'valid': ImageFolderWithPaths(valid_dir, transform=data_transforms['valid'])}\n",
    "\n",
    "dataloaders = {'train': torch.utils.data.DataLoader(image_datasets['train'], batch_size=batch_size, shuffle=True, num_workers=0, worker_init_fn=_init_fn),\n",
    "               'valid': torch.utils.data.DataLoader(image_datasets['valid'], batch_size=batch_size, shuffle=True, num_workers=0, worker_init_fn=_init_fn)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <torch.utils.data.dataloader.DataLoader at 0x17d965ebf60>,\n",
       " 'valid': <torch.utils.data.dataloader.DataLoader at 0x17d968f5cc0>}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_transforms['test'] = transforms.Compose([transforms.Resize(256, interpolation=PIL.Image.BILINEAR),\n",
    "                                                transforms.CenterCrop(224),\n",
    "                                                transforms.ToTensor(),\n",
    "                                               ])\n",
    "                   \n",
    "image_datasets['test'] = ImageFolderWithPaths(test_dir, transform=data_transforms['test'])\n",
    "\n",
    "dataloaders['test'] = torch.utils.data.DataLoader(image_datasets['test'], batch_size=batch_size, shuffle=True, num_workers=0, worker_init_fn=_init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "img_dir = os.path.join(cwd, '102flowers')\n",
    "class1_dir = os.path.join(cwd, 'class00001images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\train\\1\\image_06734.jpg\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\train\\1\\image_06735.jpg\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\train\\1\\image_06736.jpg\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\train\\1\\image_06737.jpg\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\train\\1\\image_06738.jpg\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\train\\1\\image_06739.jpg\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\train\\1\\image_06740.jpg\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\train\\1\\image_06741.jpg\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\train\\1\\image_06742.jpg\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\train\\1\\image_06743.jpg\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\train\\1\\image_06744.jpg\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\train\\1\\image_06745.jpg\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\train\\1\\image_06746.jpg\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\train\\1\\image_06747.jpg\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\train\\1\\image_06748.jpg\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\train\\1\\image_06749.jpg\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\train\\1\\image_06750.jpg\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\train\\1\\image_06751.jpg\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\train\\1\\image_06752.jpg\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\train\\1\\image_06753.jpg\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\train\\1\\image_06754.jpg\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\train\\1\\image_06755.jpg\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\train\\1\\image_06756.jpg\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\train\\1\\image_06757.jpg\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\train\\1\\image_06758.jpg\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\train\\1\\image_06759.jpg\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\train\\1\\image_06760.jpg\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\train\\1\\image_06761.jpg\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\train\\1\\image_06762.jpg\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\train\\1\\image_06763.jpg\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\train\\1\\image_06764.jpg\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\train\\1\\image_06765.jpg\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\train\\1\\image_06766.jpg\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\train\\1\\image_06767.jpg\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\train\\1\\image_06768.jpg\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\train\\1\\image_06769.jpg\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\train\\1\\image_06770.jpg\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\train\\1\\image_06771.jpg\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\train\\1\\image_06772.jpg\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\train\\1\\image_06773.jpg\n"
     ]
    }
   ],
   "source": [
    "start = 6734\n",
    "for i in range(0, 400,10):\n",
    "    imagetochoose = i + randrange(10)\n",
    "    newlocation = os.path.join(cwd, \"flower_data\\\\flower_data\\\\train\\\\1\\\\image_0\" + str(start) + \".jpg\")\n",
    "    start = start + 1\n",
    "    print(newlocation)\n",
    "    shutil.move(class1_dir + \"\\\\\" + str(imagetochoose) + \".png\", newlocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_images = 6557\n",
      "mean_R = 0.5185526868486678\n",
      "mean_G = 0.41059609417162585\n",
      "mean_B = 0.328709264728159\n",
      "std_R = 0.29711427467272\n",
      "std_G = 0.24972758964352312\n",
      "std_B = 0.2851732337610563\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataloader = dataloaders['train']  # Calc mean and std dev on valid dataset.\n",
    "\n",
    "num_batches = len(dataloader)\n",
    "num_images = len(dataloader.dataset)\n",
    "\n",
    "print(\"num_images =\", num_images)\n",
    "\n",
    "sum_R = 0.0\n",
    "sum_G = 0.0\n",
    "sum_B = 0.0\n",
    "\n",
    "# MEAN \n",
    "for batch_idx, (images, labels, paths) in enumerate(dataloader):\n",
    "    for image in images:\n",
    "        numpy_image = image.numpy()\n",
    "        \n",
    "        sum_R += np.mean(numpy_image[0, :, :])\n",
    "        sum_G += np.mean(numpy_image[1, :, :])\n",
    "        sum_B += np.mean(numpy_image[2, :, :])\n",
    "        \n",
    "        \n",
    "mean_R = sum_R / num_images\n",
    "mean_G = sum_G / num_images\n",
    "mean_B = sum_B / num_images\n",
    "\n",
    "print(\"mean_R =\", mean_R)\n",
    "print(\"mean_G =\", mean_G)\n",
    "print(\"mean_B =\", mean_B)\n",
    "\n",
    "variance_sum_R = 0.0\n",
    "variance_sum_G = 0.0\n",
    "variance_sum_B = 0.0\n",
    "\n",
    "# STD\n",
    "for batch_idx, (images, labels, paths) in enumerate(dataloader):\n",
    "    for image in images:\n",
    "        numpy_image = image.numpy()\n",
    "        \n",
    "        variance_sum_R += np.mean(np.square(numpy_image[0, :, :] - mean_R))\n",
    "        variance_sum_G += np.mean(np.square(numpy_image[1, :, :] - mean_G))\n",
    "        variance_sum_B += np.mean(np.square(numpy_image[2, :, :] - mean_B))\n",
    "\n",
    "\n",
    "std_R = math.sqrt(variance_sum_R / num_images)\n",
    "std_G = math.sqrt(variance_sum_G / num_images)\n",
    "std_B = math.sqrt(variance_sum_B / num_images)\n",
    "\n",
    "print(\"std_R =\", std_R)\n",
    "print(\"std_G =\", std_G)\n",
    "print(\"std_B =\", std_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm_mean = [0.5178361839861569, 0.4106749456881299, 0.32864167836880803]\n",
    "norm_std = [0.2972239085211309, 0.24976049135203868, 0.28533308036347665]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_weights_for_balanced_classes(images, nclasses):                        \n",
    "    count = [0] * nclasses   \n",
    "    \n",
    "    # Counts per label\n",
    "    for item in images:                                                         \n",
    "        count[item[1]] += 1                                                     \n",
    "    \n",
    "    weight_per_class = [0.] * nclasses\n",
    "    \n",
    "    # Total number of images.\n",
    "    N = float(sum(count))                                                   \n",
    "    \n",
    "    # super-sample the smaller classes.\n",
    "    for i in range(nclasses):                                                   \n",
    "        weight_per_class[i] = N/float(count[i])                                 \n",
    "    \n",
    "    weight = [0] * len(images)                                              \n",
    "    \n",
    "    # Calculate a weight per image.\n",
    "    for idx, val in enumerate(images):                                          \n",
    "        weight[idx] = weight_per_class[val[1]]                                  \n",
    "    \n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using the image datasets and the trainforms, define the dataloaders\n",
    "batch_size = 32\n",
    "\n",
    "# Transforms for the training and validation sets\n",
    "data_transforms = {'train': transforms.Compose([# transforms.Resize(256, interpolation=PIL.Image.BILINEAR),\n",
    "                                                # transforms.CenterCrop(224),\n",
    "                                                #===\n",
    "    \n",
    "                                                # transforms.RandomResizedCrop(224, \n",
    "                                                #                              # scale=(0.75, 1.0), ratio=(1.0, 1.0), \n",
    "                                                #                              interpolation=PIL.Image.BILINEAR),\n",
    "                                                transforms.RandomAffine(45, translate=(0.4, 0.4), scale=(0.75, 1.5), shear=None, resample=PIL.Image.BILINEAR, fillcolor=0),\n",
    "                                                transforms.Resize(256, interpolation=PIL.Image.BILINEAR),\n",
    "                                                transforms.CenterCrop(224),\n",
    "                                                \n",
    "                                                # transforms.RandomHorizontalFlip(),\n",
    "                                                # transforms.RandomVerticalFlip(),\n",
    "                                                # transforms.ColorJitter(brightness=0.2, contrast=0.1, saturation=0.0, hue=0.1),\n",
    "                                                # transforms.ColorJitter(brightness=0.025, contrast=0.0, saturation=0.0, hue=0.025),\n",
    "                                                # transforms.RandomGrayscale(p=0.3),\n",
    "                                                # transforms.RandomRotation(45.0, resample=PIL.Image.BILINEAR),\n",
    "                                                #===\n",
    "                                                # transforms.Grayscale(num_output_channels=3),\n",
    "                                                transforms.ToTensor(),\n",
    "                                                transforms.Normalize(norm_mean, norm_std),\n",
    "                                               ]),\n",
    "                   'valid': transforms.Compose([# transforms.RandomRotation(45, resample=PIL.Image.BILINEAR),\n",
    "                                                transforms.Resize(256, interpolation=PIL.Image.BILINEAR),\n",
    "                                                transforms.CenterCrop(224),\n",
    "                                                # transforms.RandomHorizontalFlip(),\n",
    "                                                #===\n",
    "                                                #===\n",
    "                                                # transforms.Grayscale(num_output_channels=3),\n",
    "                                                transforms.ToTensor(),\n",
    "                                                transforms.Normalize(norm_mean, norm_std),\n",
    "                                               ])}\n",
    "\n",
    "                   \n",
    "# Load the datasets with ImageFolder\n",
    "image_datasets = {'train': ImageFolderWithPaths(train_dir, transform=data_transforms['train']),\n",
    "                  'valid': ImageFolderWithPaths(valid_dir, transform=data_transforms['valid'])}\n",
    "\n",
    "weights = make_weights_for_balanced_classes(image_datasets['train'].imgs, len(image_datasets['train'].classes))                                                                \n",
    "weights = torch.DoubleTensor(weights)                                       \n",
    "sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(weights))                     \n",
    "                                                                                \n",
    "  \n",
    "dataloaders = {'train': torch.utils.data.DataLoader(image_datasets['train'], batch_size=batch_size, sampler = sampler, num_workers=0, worker_init_fn=_init_fn),\n",
    "               'valid': torch.utils.data.DataLoader(image_datasets['valid'], batch_size=batch_size, shuffle=True, num_workers=0, worker_init_fn=_init_fn)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_transforms['test'] = transforms.Compose([# transforms.RandomRotation(45, resample=PIL.Image.BILINEAR),\n",
    "                                              transforms.Resize(256, interpolation=PIL.Image.BILINEAR),\n",
    "                                              transforms.CenterCrop(224),\n",
    "                                              # transforms.RandomHorizontalFlip(),\n",
    "                                              #===\n",
    "                                              #===\n",
    "                                              # transforms.Grayscale(num_output_channels=3),\n",
    "                                              transforms.ToTensor(),\n",
    "                                              transforms.Normalize(norm_mean, norm_std),\n",
    "                                             ])\n",
    "\n",
    "# Load the datasets with ImageFolder\n",
    "image_datasets['test'] = ImageFolderWithPaths(test_dir, transform=data_transforms['test'])\n",
    "\n",
    "dataloaders['test'] = torch.utils.data.DataLoader(image_datasets['test'], batch_size=batch_size, shuffle=True, num_workers=5, worker_init_fn=_init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def imshow_numpy(image, ax=None, title=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(10,8))\n",
    "        \n",
    "    ax.grid(False)\n",
    "\n",
    "    # PyTorch tensors assume the color channel is the first dimension\n",
    "    # but matplotlib assumes is the third dimension\n",
    "    image = image.transpose((1, 2, 0))\n",
    "    \n",
    "    # Undo preprocessing\n",
    "    mean = np.array(norm_mean)\n",
    "    std = np.array(norm_std)\n",
    "    image = std * image + mean\n",
    "    \n",
    "    # Image needs to be clipped between 0 and 1 or it looks like noise when displayed\n",
    "    image = np.clip(image, 0, 1)\n",
    "    \n",
    "    ax.imshow(image)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(dataloaders['train'].dataset) = 6565\n",
      "len(dataloaders['valid'].dataset) = 818\n"
     ]
    }
   ],
   "source": [
    "print(\"len(dataloaders['train'].dataset) =\", len(dataloaders['train'].dataset))\n",
    "print(\"len(dataloaders['valid'].dataset) =\", len(dataloaders['valid'].dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(dataloaders['test'].dataset) = 846\n"
     ]
    }
   ],
   "source": [
    "print(\"len(dataloaders['test'].dataset) =\", len(dataloaders['test'].dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\train\\\\55\\\\image_04710.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\train\\\\87\\\\image_05521.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\train\\\\28\\\\image_05260.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\train\\\\12\\\\image_04066.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\train\\\\35\\\\image_06994.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\train\\\\101\\\\image_07945.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\train\\\\68\\\\image_05943.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\train\\\\28\\\\image_05235.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\train\\\\32\\\\image_05624.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\train\\\\29\\\\image_04134.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\train\\\\12\\\\image_04026.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\train\\\\58\\\\image_02720.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\train\\\\15\\\\image_06392.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\train\\\\82\\\\image_01632.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\train\\\\4\\\\image_05642.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\train\\\\9\\\\image_06436.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\train\\\\38\\\\image_05804.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\train\\\\96\\\\image_07671.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\train\\\\44\\\\image_01526.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\train\\\\53\\\\image_03706.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\train\\\\69\\\\image_06009.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\train\\\\33\\\\image_06453.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\train\\\\1\\\\image_06745.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\train\\\\12\\\\image_04061.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\train\\\\85\\\\image_04796.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\train\\\\59\\\\image_05022.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\train\\\\45\\\\image_07157.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\train\\\\24\\\\image_06833.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\train\\\\85\\\\image_04803.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\train\\\\18\\\\image_04266.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\train\\\\16\\\\image_06653.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\train\\\\95\\\\image_07590.jpg')\n",
      "('C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\valid\\\\88\\\\image_00508.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\valid\\\\10\\\\image_07102.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\valid\\\\15\\\\image_06358.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\valid\\\\63\\\\image_05867.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\valid\\\\64\\\\image_06129.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\valid\\\\91\\\\image_04878.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\valid\\\\6\\\\image_08105.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\valid\\\\61\\\\image_06292.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\valid\\\\59\\\\image_05056.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\valid\\\\65\\\\image_03224.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\valid\\\\55\\\\image_04720.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\valid\\\\35\\\\image_07005.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\valid\\\\76\\\\image_02523.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\valid\\\\13\\\\image_05772.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\valid\\\\60\\\\image_02928.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\valid\\\\18\\\\image_04261.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\valid\\\\72\\\\image_03584.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\valid\\\\32\\\\image_05584.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\valid\\\\82\\\\image_01693.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\valid\\\\100\\\\image_07917.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\valid\\\\41\\\\image_02278.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\valid\\\\84\\\\image_02598.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\valid\\\\55\\\\image_04722.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\valid\\\\82\\\\image_01683.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\valid\\\\75\\\\image_02147.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\valid\\\\55\\\\image_04753.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\valid\\\\18\\\\image_04312.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\valid\\\\95\\\\image_07471.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\valid\\\\18\\\\image_04290.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\valid\\\\46\\\\image_01034.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\valid\\\\47\\\\image_05007.jpg', 'C:\\\\jeff\\\\senior\\\\research\\\\text-to-image\\\\flower_data\\\\flower_data\\\\valid\\\\97\\\\image_07745.jpg')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x248ba6d84a8>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images, labels, paths = next(iter(dataloaders['train']))\n",
    "grid_images = utils.make_grid(images)                            \n",
    "print(paths)\n",
    "imshow_numpy(grid_images.numpy())\n",
    "\n",
    "\n",
    "images, labels, paths = next(iter(dataloaders['valid']))\n",
    "grid_images = utils.make_grid(images)                            \n",
    "print(paths)\n",
    "imshow_numpy(grid_images.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def progress_bar(value, max=100):\n",
    "    return HTML(\"\"\"\n",
    "        <progress\n",
    "            value='{value}'\n",
    "            max='{max}',\n",
    "            style='width: 100%'\n",
    "        >\n",
    "            {value}\n",
    "        </progress>\n",
    "    \"\"\".format(value=value, max=max))\n",
    "\n",
    "  \n",
    "valid_loss_min_A = np.Inf\n",
    "valid_acc_max_A = 0\n",
    "\n",
    "valid_loss_min_B = np.Inf\n",
    "valid_acc_max_B = 0\n",
    "\n",
    "train_losses, valid_losses = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FFClassifier(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x      \n",
    "      \n",
    "      \n",
    "def save_checkpoint(checkpoint_path, model):\n",
    "    checkpoint = {\n",
    "        \"class_to_idx\": model.class_to_idx,        \n",
    "        \"idx_to_class\": model.idx_to_class,        \n",
    "        \"cat_to_name\": model.cat_to_name,        \n",
    "        \"state_dict\": model.state_dict()\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    \n",
    "    \n",
    "def load_checkpoint(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "\n",
    "    model = models.resnet152(pretrained=False)\n",
    "    \n",
    "    for param in model.parameters():\n",
    "       param.requires_grad = False\n",
    "\n",
    "    # Put the classifier on the pretrained network\n",
    "    model.fc = FFClassifier(2048, 102)\n",
    "\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.class_to_idx = checkpoint[\"class_to_idx\"]\n",
    "    model.idx_to_class = checkpoint[\"idx_to_class\"]\n",
    "    model.cat_to_name = checkpoint[\"cat_to_name\"]\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model.\n",
    "model = models.resnet152(pretrained=True)\n",
    "model_requires_grad_params = []\n",
    "\n",
    "# Freeze parameters so we don't backprop through them\n",
    "for param in model.parameters():\n",
    "  if param.requires_grad == True:\n",
    "      # param.requires_grad = False\n",
    "      model_requires_grad_params.append(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fc = FFClassifier(2048, 102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "generic_type: cannot initialize type \"_CudaDeviceProperties\": an object with that name is already defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-65ca5047f7ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# specify loss function (categorical cross-entropy)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mcuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    247\u001b[0m             \u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m         \"\"\"\n\u001b[1;32m--> 249\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    180\u001b[0m                 \u001b[1;31m# Tensors stored in modules are graph leaves, and we don't\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m                 \u001b[1;31m# want to create copy nodes, so we have to unpack the data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m                 \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    247\u001b[0m             \u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m         \"\"\"\n\u001b[1;32m--> 249\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    159\u001b[0m             \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[0;32m    160\u001b[0m     \u001b[0m_check_driver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m     \u001b[0m_cudart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[0m_cudart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudaGetErrorName\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: generic_type: cannot initialize type \"_CudaDeviceProperties\": an object with that name is already defined"
     ]
    }
   ],
   "source": [
    "model.cuda()\n",
    "    \n",
    "num_epochs = 20\n",
    "        \n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# specify optimizer\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    [\n",
    "        {'params': model.conv1.parameters(),  'lr': 0.000001},\n",
    "        {'params': model.layer1.parameters(), 'lr': 0.000001},\n",
    "        {'params': model.layer2.parameters(), 'lr': 0.00001},\n",
    "        {'params': model.layer3.parameters(), 'lr': 0.00001},\n",
    "        {'params': model.layer4.parameters(), 'lr': 0.0001},\n",
    "        {'params': model.fc.parameters(),     'lr': 0.001}\n",
    "    ], lr=0.0, weight_decay=0.001)\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10, 20, 30, 40], gamma=0.3)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.3)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=10, verbose=True)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "for idx, param_group in enumerate(optimizer.param_groups):\n",
    "    print(idx, param_group['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(n_epochs = 10, manual_lrs = None, run_schedular = True):\n",
    "    # Need access to some global notebook variables...\n",
    "    global valid_loss_min_A\n",
    "    global valid_acc_max_A\n",
    "    global valid_loss_min_B\n",
    "    global valid_acc_max_B    \n",
    "    global train_losses\n",
    "    global valid_losses\n",
    "\n",
    "    # Update the param group learning rates if supplied.\n",
    "    if manual_lrs is not None:\n",
    "        for idx, param_group in enumerate(optimizer.param_groups):\n",
    "            param_group['lr'] = manual_lrs[idx]\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        print(\"Epoch {}\".format(epoch))\n",
    "                \n",
    "        if run_schedular:\n",
    "            scheduler.step()\n",
    "\n",
    "        max_lr = 0.0  # max current lr over param groups.\n",
    "\n",
    "        for param_group in optimizer.param_groups:\n",
    "            if param_group['lr'] > max_lr:\n",
    "              max_lr = param_group['lr']\n",
    "\n",
    "        print(\"max_lr = {}\".format(max_lr))\n",
    "        \n",
    "        # print(\"LRs\", scheduler.get_lr())\n",
    "\n",
    "        train_loss_sum = 0.0\n",
    "        valid_loss_sum = 0.0\n",
    "        train_correct_count = 0.0\n",
    "        valid_correct_count = 0.0\n",
    "\n",
    "        \n",
    "        ##############################################\n",
    "        # Choose the training and validation datasets.\n",
    "        \n",
    "        train_dataloader = dataloaders['train']\n",
    "        # train_dataloader = super_train_dataloader\n",
    "\n",
    "        valid_dataloader = dataloaders['valid']\n",
    "        # valid_dataloader = dataloaders['test']\n",
    "        ##############################################\n",
    "\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        print(\"Training...\")\n",
    "\n",
    "        training_start_time = time.time()\n",
    "        train_display = display(progress_bar(0, 100), display_id=True)\n",
    "        model.train()    \n",
    "        \n",
    "        num_batches = math.ceil(len(train_dataloader.dataset) / batch_size)\n",
    "\n",
    "        for batch_idx, (images, labels, paths) in enumerate(train_dataloader):\n",
    "            # labels are the integer indexes of the class/folder names.\n",
    "\n",
    "            if cuda_is_available:\n",
    "                images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(images)  # batch_size x 102\n",
    "                loss = criterion(outputs, labels)  # Average loss value over batch.\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            train_loss_sum += loss.item() * images.size(0)  \n",
    "\n",
    "            _, predicted_labels = torch.max(outputs, -1)\n",
    "            train_correct_count += (predicted_labels == labels).double().sum().item()   \n",
    "\n",
    "            progress = (batch_idx+1) * 100.0 / num_batches        \n",
    "            train_display.update(progress_bar(progress, 100))\n",
    "\n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        print(\"Validating...\")\n",
    "\n",
    "        validation_start_time = time.time()\n",
    "        valid_display = display(progress_bar(0, 100), display_id=True)\n",
    "        model.eval()    \n",
    "        \n",
    "        num_batches = math.ceil(len(valid_dataloader.dataset) / batch_size)\n",
    "\n",
    "        for batch_idx, (images, labels, paths) in enumerate(valid_dataloader):\n",
    "            if cuda_is_available:\n",
    "                images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "            with torch.set_grad_enabled(False):\n",
    "                outputs = model(images)  # batch_size x 102\n",
    "                loss = criterion(outputs, labels)  # Average loss value over batch.\n",
    "\n",
    "            valid_loss_sum += loss.item() * images.size(0)\n",
    "\n",
    "            _, predicted_labels = torch.max(outputs, -1)\n",
    "            valid_correct_count += (predicted_labels == labels).double().sum().item()\n",
    "\n",
    "            progress = (batch_idx+1) * 100.0 / num_batches\n",
    "            valid_display.update(progress_bar(progress, 100))\n",
    "\n",
    "\n",
    "        epoch_end_time = time.time()\n",
    "\n",
    "        ############################\n",
    "        # calculate average losses #\n",
    "        ############################\n",
    "        train_loss = train_loss_sum / len(train_dataloader.dataset)\n",
    "        valid_loss = valid_loss_sum / len(valid_dataloader.dataset)\n",
    "        train_acc = train_correct_count / len(train_dataloader.dataset)\n",
    "        valid_acc = valid_correct_count / len(valid_dataloader.dataset)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "\n",
    "\n",
    "        #if run_schedular:\n",
    "        #    scheduler.step(valid_loss)\n",
    "\n",
    "            \n",
    "        print('Training Loss={:.6f}  Training Accuracy={:.6f}  Duration={:.2f}'.format(train_loss, \n",
    "                                                                                       train_acc, \n",
    "                                                                                       validation_start_time - training_start_time))  \n",
    "\n",
    "        print('Validation Loss={:.6f}  Validation Accuracy={:.6f}  Duration={:.2f}'.format(valid_loss, \n",
    "                                                                                           valid_acc, \n",
    "                                                                                           epoch_end_time - validation_start_time))\n",
    "\n",
    "\n",
    "        if (valid_loss < valid_loss_min_A) or ((valid_loss == valid_loss_min_A) and (valid_acc >= valid_acc_max_A)):\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min_A, valid_loss))\n",
    "            torch.save(model.state_dict(), 'state_dict_best_valid_loss.pt')\n",
    "            valid_loss_min_A = valid_loss\n",
    "            valid_acc_max_A = valid_acc\n",
    "\n",
    "            \n",
    "        if (valid_acc > valid_acc_max_B) or ((valid_acc == valid_acc_max_B) and (valid_loss <= valid_loss_min_B)):\n",
    "            print('Validation acc increased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_acc_max_B, valid_acc))\n",
    "            torch.save(model.state_dict(), 'state_dict_best_valid_acc.pt')\n",
    "            valid_loss_min_B = valid_loss\n",
    "            valid_acc_max_B = valid_acc\n",
    "\n",
    "\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cuda_is_available = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "max_lr = 0.001\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='100.0'\n",
       "            max='100',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            100.0\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='100.0'\n",
       "            max='100',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            100.0\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss=2.581765  Training Accuracy=0.484387  Duration=320.76\n",
      "Validation Loss=1.063241  Validation Accuracy=0.786064  Duration=32.41\n",
      "Validation loss decreased (inf --> 1.063241).  Saving model ...\n",
      "Validation acc increased (0.000000 --> 0.786064).  Saving model ...\n",
      "\n",
      "Epoch 2\n",
      "max_lr = 0.001\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='100.0'\n",
       "            max='100',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            100.0\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='100.0'\n",
       "            max='100',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            100.0\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss=1.018465  Training Accuracy=0.794669  Duration=318.32\n",
      "Validation Loss=0.628612  Validation Accuracy=0.855746  Duration=29.02\n",
      "Validation loss decreased (1.063241 --> 0.628612).  Saving model ...\n",
      "Validation acc increased (0.786064 --> 0.855746).  Saving model ...\n",
      "\n",
      "Epoch 3\n",
      "max_lr = 0.001\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='100.0'\n",
       "            max='100',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            100.0\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='100.0'\n",
       "            max='100',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            100.0\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss=0.757337  Training Accuracy=0.830465  Duration=321.32\n",
      "Validation Loss=0.414580  Validation Accuracy=0.911980  Duration=28.60\n",
      "Validation loss decreased (0.628612 --> 0.414580).  Saving model ...\n",
      "Validation acc increased (0.855746 --> 0.911980).  Saving model ...\n",
      "\n",
      "Epoch 4\n",
      "max_lr = 0.001\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='100.0'\n",
       "            max='100',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            100.0\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='100.0'\n",
       "            max='100',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            100.0\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss=0.615897  Training Accuracy=0.850724  Duration=321.32\n",
      "Validation Loss=0.419523  Validation Accuracy=0.883863  Duration=28.86\n",
      "\n",
      "Epoch 5\n",
      "max_lr = 0.001\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='100.0'\n",
       "            max='100',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            100.0\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='100.0'\n",
       "            max='100',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            100.0\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss=0.527259  Training Accuracy=0.877685  Duration=320.58\n",
      "Validation Loss=0.331020  Validation Accuracy=0.916870  Duration=28.71\n",
      "Validation loss decreased (0.414580 --> 0.331020).  Saving model ...\n",
      "Validation acc increased (0.911980 --> 0.916870).  Saving model ...\n",
      "\n",
      "Epoch 6\n",
      "max_lr = 0.001\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='100.0'\n",
       "            max='100',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            100.0\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='100.0'\n",
       "            max='100',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            100.0\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss=0.513289  Training Accuracy=0.871592  Duration=319.82\n",
      "Validation Loss=0.341838  Validation Accuracy=0.921760  Duration=28.93\n",
      "Validation acc increased (0.916870 --> 0.921760).  Saving model ...\n",
      "\n",
      "Epoch 7\n",
      "max_lr = 0.001\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='100.0'\n",
       "            max='100',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            100.0\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='100.0'\n",
       "            max='100',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            100.0\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss=0.493956  Training Accuracy=0.877380  Duration=324.53\n",
      "Validation Loss=0.294651  Validation Accuracy=0.930318  Duration=28.86\n",
      "Validation loss decreased (0.331020 --> 0.294651).  Saving model ...\n",
      "Validation acc increased (0.921760 --> 0.930318).  Saving model ...\n",
      "\n",
      "Epoch 8\n",
      "max_lr = 0.001\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='100.0'\n",
       "            max='100',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            100.0\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='100.0'\n",
       "            max='100',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            100.0\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss=0.458716  Training Accuracy=0.883473  Duration=321.44\n",
      "Validation Loss=0.274666  Validation Accuracy=0.935208  Duration=28.82\n",
      "Validation loss decreased (0.294651 --> 0.274666).  Saving model ...\n",
      "Validation acc increased (0.930318 --> 0.935208).  Saving model ...\n",
      "\n",
      "Epoch 9\n",
      "max_lr = 0.001\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='100.0'\n",
       "            max='100',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            100.0\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='100.0'\n",
       "            max='100',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            100.0\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss=0.431077  Training Accuracy=0.888195  Duration=321.85\n",
      "Validation Loss=0.254562  Validation Accuracy=0.936430  Duration=28.95\n",
      "Validation loss decreased (0.274666 --> 0.254562).  Saving model ...\n",
      "Validation acc increased (0.935208 --> 0.936430).  Saving model ...\n",
      "\n",
      "Epoch 10\n",
      "max_lr = 0.001\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='100.0'\n",
       "            max='100',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            100.0\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='100.0'\n",
       "            max='100',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            100.0\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss=0.403241  Training Accuracy=0.897944  Duration=339.02\n",
      "Validation Loss=0.275592  Validation Accuracy=0.930318  Duration=30.35\n",
      "\n",
      "Epoch 11\n",
      "max_lr = 0.001\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='100.0'\n",
       "            max='100',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            100.0\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='100.0'\n",
       "            max='100',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            100.0\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss=0.423604  Training Accuracy=0.889718  Duration=342.01\n",
      "Validation Loss=0.257287  Validation Accuracy=0.936430  Duration=29.08\n",
      "\n",
      "Epoch 12\n",
      "max_lr = 0.001\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='100.0'\n",
       "            max='100',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            100.0\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='100.0'\n",
       "            max='100',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            100.0\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss=0.397387  Training Accuracy=0.896116  Duration=344.29\n",
      "Validation Loss=0.263608  Validation Accuracy=0.929095  Duration=29.30\n",
      "\n",
      "Epoch 13\n",
      "max_lr = 0.001\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='100.0'\n",
       "            max='100',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            100.0\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='100.0'\n",
       "            max='100',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            100.0\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss=0.400596  Training Accuracy=0.893069  Duration=338.97\n",
      "Validation Loss=0.248586  Validation Accuracy=0.936430  Duration=29.29\n",
      "Validation loss decreased (0.254562 --> 0.248586).  Saving model ...\n",
      "Validation acc increased (0.936430 --> 0.936430).  Saving model ...\n",
      "\n",
      "Epoch 14\n",
      "max_lr = 0.001\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='100.0'\n",
       "            max='100',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            100.0\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='100.0'\n",
       "            max='100',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            100.0\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss=0.385870  Training Accuracy=0.899162  Duration=328.73\n",
      "Validation Loss=0.231623  Validation Accuracy=0.941320  Duration=28.70\n",
      "Validation loss decreased (0.248586 --> 0.231623).  Saving model ...\n",
      "Validation acc increased (0.936430 --> 0.941320).  Saving model ...\n",
      "\n",
      "Epoch 15\n",
      "max_lr = 0.001\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='100.0'\n",
       "            max='100',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            100.0\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='100.0'\n",
       "            max='100',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            100.0\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss=0.408765  Training Accuracy=0.890632  Duration=320.18\n",
      "Validation Loss=0.299999  Validation Accuracy=0.913203  Duration=28.74\n",
      "\n",
      "Epoch 16\n",
      "max_lr = 0.001\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='100.0'\n",
       "            max='100',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            100.0\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='100.0'\n",
       "            max='100',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            100.0\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss=0.407498  Training Accuracy=0.886367  Duration=322.27\n",
      "Validation Loss=0.252000  Validation Accuracy=0.935208  Duration=28.53\n",
      "\n",
      "Epoch 17\n",
      "max_lr = 0.001\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='100.0'\n",
       "            max='100',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            100.0\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='100.0'\n",
       "            max='100',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            100.0\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss=0.372627  Training Accuracy=0.904494  Duration=319.50\n",
      "Validation Loss=0.275470  Validation Accuracy=0.927873  Duration=28.72\n",
      "\n",
      "Epoch 18\n",
      "max_lr = 0.001\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='100.0'\n",
       "            max='100',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            100.0\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='100.0'\n",
       "            max='100',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            100.0\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss=0.368995  Training Accuracy=0.900381  Duration=318.93\n",
      "Validation Loss=0.267496  Validation Accuracy=0.935208  Duration=28.41\n",
      "\n",
      "Epoch 19\n",
      "max_lr = 0.001\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='100.0'\n",
       "            max='100',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            100.0\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='100.0'\n",
       "            max='100',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            100.0\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss=0.384730  Training Accuracy=0.901295  Duration=317.75\n",
      "Validation Loss=0.232595  Validation Accuracy=0.944988  Duration=28.54\n",
      "Validation acc increased (0.941320 --> 0.944988).  Saving model ...\n",
      "\n",
      "Epoch 20\n",
      "max_lr = 0.001\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='100.0'\n",
       "            max='100',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            100.0\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='100.0'\n",
       "            max='100',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            100.0\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss=0.339399  Training Accuracy=0.906778  Duration=317.90\n",
      "Validation Loss=0.217990  Validation Accuracy=0.940098  Duration=28.67\n",
      "Validation loss decreased (0.231623 --> 0.217990).  Saving model ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for param in model_requires_grad_params:\n",
    "  param.requires_grad = False\n",
    "\n",
    "# Set smaller learning rates for the earlier layers of the model.\n",
    "manual_lrs = [0.000001, \n",
    "              0.000001, \n",
    "              0.00001, \n",
    "              0.00001, \n",
    "              0.0001, \n",
    "              0.001]\n",
    "\n",
    "train(20, manual_lrs, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-f77bb9c465f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'state_dict_best_valid_loss.pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[0;32m    301\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 303\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    304\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_load\u001b[1;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[0;32m    467\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    468\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 469\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    470\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m     \u001b[0mdeserialized_storage_keys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m    435\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mroot_key\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m                 deserialized_objects[root_key] = restore_location(\n\u001b[1;32m--> 437\u001b[1;33m                     data_type(size), location)\n\u001b[0m\u001b[0;32m    438\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mroot_key\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mview_metadata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[1;34m(obj, location)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cuda'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[1;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprev_idx\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_setDevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m         \u001b[0m_lazy_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[0m_check_driver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m     \u001b[0m_cudart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m     \u001b[0m_cudart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudaGetErrorName\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[0m_cudart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudaGetErrorString\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_load_cudart\u001b[1;34m()\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;31m# First check the main program for CUDA symbols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mplatform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Windows'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[0mlib\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_cuda_windows_lib\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mlib\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcdll\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLoadLibrary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36mfind_cuda_windows_lib\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfind_cuda_windows_lib\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mproc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'where'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cudart64*.dll'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors)\u001b[0m\n\u001b[0;32m    663\u001b[0m         (p2cread, p2cwrite,\n\u001b[0;32m    664\u001b[0m          \u001b[0mc2pread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 665\u001b[1;33m          errread, errwrite) = self._get_handles(stdin, stdout, stderr)\n\u001b[0m\u001b[0;32m    666\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    667\u001b[0m         \u001b[1;31m# We wrap OS handles *before* launching the child, otherwise a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_get_handles\u001b[1;34m(self, stdin, stdout, stderr)\u001b[0m\n\u001b[0;32m    898\u001b[0m                 \u001b[1;31m# Assuming file-like object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m                 \u001b[0mp2cread\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmsvcrt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_osfhandle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstdin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfileno\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 900\u001b[1;33m             \u001b[0mp2cread\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_inheritable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp2cread\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    901\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstdout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_make_inheritable\u001b[1;34m(self, handle)\u001b[0m\n\u001b[0;32m    948\u001b[0m                 \u001b[0m_winapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGetCurrentProcess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m                 \u001b[0m_winapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGetCurrentProcess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m                 _winapi.DUPLICATE_SAME_ACCESS)\n\u001b[0m\u001b[0;32m    951\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mHandle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('state_dict_best_valid_loss.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "max_lr = 0.001\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='0'\n",
       "            max='100',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            0\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at c:\\programdata\\miniconda3\\conda-bld\\pytorch_1524543037166\\work\\aten\\src\\thc\\generic/THCStorage.cu:58",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-9199da4dfc21>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m               0.001]\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmanual_lrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-47-f33ed92afcc4>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(n_epochs, manual_lrs, run_schedular)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m                 \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# batch_size x 102\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Average loss value over batch.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\models\\resnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\models\\resnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     47\u001b[0m         return F.batch_norm(\n\u001b[0;32m     48\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             self.training or not self.track_running_stats, self.momentum, self.eps)\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   1192\u001b[0m     return torch.batch_norm(\n\u001b[0;32m   1193\u001b[0m         \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1194\u001b[1;33m         \u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m     )\n\u001b[0;32m   1196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at c:\\programdata\\miniconda3\\conda-bld\\pytorch_1524543037166\\work\\aten\\src\\thc\\generic/THCStorage.cu:58"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# UN-Freeze the earlier layers of the pre-trained network!\n",
    "for param in model_requires_grad_params:\n",
    "  param.requires_grad = True\n",
    "\n",
    "# Set smaller learning rates for the earlier layers of the model.\n",
    "manual_lrs = [0.000001, \n",
    "              0.000001, \n",
    "              0.00001, \n",
    "              0.00001, \n",
    "              0.0001, \n",
    "              0.001]\n",
    "\n",
    "train(num_epochs, manual_lrs, True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x248cf011f98>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(valid_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ResNet:\n\tWhile copying the parameter named \"conv1.weight\", whose dimensions in the model are torch.Size([64, 3, 7, 7]) and whose dimensions in the checkpoint are torch.Size([64, 3, 7, 7]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-f77bb9c465f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'state_dict_best_valid_loss.pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m    719\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    720\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[1;32m--> 721\u001b[1;33m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[0;32m    722\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    723\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ResNet:\n\tWhile copying the parameter named \"conv1.weight\", whose dimensions in the model are torch.Size([64, 3, 7, 7]) and whose dimensions in the checkpoint are torch.Size([64, 3, 7, 7])."
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('state_dict_best_valid_loss.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx_to_class = {v: k for k, v in image_datasets['train'].class_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# idx is the same as the label.\n",
    "# class is the folder name ['1'...'102']\n",
    "\n",
    "model.class_to_idx = image_datasets['train'].class_to_idx\n",
    "model.idx_to_class = idx_to_class\n",
    "model.cat_to_name = cat_to_name\n",
    "\n",
    "# Save the checkpoint \n",
    "save_checkpoint('model_checkpoint.pt', model)\n",
    "\n",
    "# Also save to gdrive.\n",
    "ts = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')\n",
    "checkpointsave = os.path.join(cwd, 'checkpoint', 'model_checkpoint_B_10-29-2019.pt')\n",
    "save_checkpoint(checkpointsave, model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_checkpoint('model_checkpoint.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'progress_bar' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-e2d4ccbb7504>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mvalidation_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Validating...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mvalid_display\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisplay_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'progress_bar' is not defined"
     ]
    }
   ],
   "source": [
    "valid_loss_sum = 0.0\n",
    "valid_correct_count = 0.0\n",
    "\n",
    "######################    \n",
    "# validate the model #\n",
    "######################\n",
    "validation_start_time = time.time()\n",
    "print(\"Validating...\")\n",
    "valid_display = display(progress_bar(0, 100), display_id=True)\n",
    "\n",
    "model.eval()    \n",
    "\n",
    "if cuda_is_available:\n",
    "    model.cuda()\n",
    "\n",
    "num_batches = math.ceil(len(dataloaders['valid'].dataset) / batch_size)\n",
    "\n",
    "for batch_idx, (images, labels, paths) in enumerate(dataloaders['valid']):\n",
    "\n",
    "    if cuda_is_available:\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        outputs = model(images)  # batch_size x 102\n",
    "        loss = criterion(outputs, labels)  # Average loss value over batch.\n",
    "\n",
    "    valid_loss_sum += loss.item() * images.size(0)\n",
    "\n",
    "    _, predicted_labels = torch.max(outputs, -1)\n",
    "    valid_correct_count += (predicted_labels == labels).double().sum().item()\n",
    "        \n",
    "    for i in range(len(labels.data)):\n",
    "        if labels[i] != predicted_labels[i]:\n",
    "            print(paths[i], \" truth =\", idx_to_class[labels[i].item()],\" predicted =\", idx_to_class[predicted_labels[i].item()])\n",
    "\n",
    "    progress = (batch_idx+1) * 100.0 / num_batches\n",
    "    valid_display.update(progress_bar(progress, 100))\n",
    "\n",
    "\n",
    "validation_end_time = time.time()\n",
    "\n",
    "############################\n",
    "# calculate average losses #\n",
    "############################\n",
    "valid_loss = valid_loss_sum / len(dataloaders['valid'].dataset)\n",
    "valid_acc = valid_correct_count / len(dataloaders['valid'].dataset)\n",
    "\n",
    "print('Validation Loss={:.6f}  Validation Accuracy={:.6f}  Duration={:.2f}'.format(valid_loss, \n",
    "                                                                                   valid_acc, \n",
    "                                                                                   validation_end_time - validation_start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='100.0'\n",
       "            max='100',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            100.0\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\101\\image_07988.jpg  truth = 101  predicted = 58\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\100\\image_07897.jpg  truth = 100  predicted = 41\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\74\\image_01276.jpg  truth = 74  predicted = 4\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\25\\image_06583.jpg  truth = 25  predicted = 22\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\84\\image_02572.jpg  truth = 84  predicted = 65\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\48\\image_04667.jpg  truth = 48  predicted = 65\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\1\\image_06752.jpg  truth = 1  predicted = 65\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\1\\image_06770.jpg  truth = 1  predicted = 86\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\51\\image_03989.jpg  truth = 51  predicted = 86\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\83\\image_01759.jpg  truth = 83  predicted = 43\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\20\\image_04910.jpg  truth = 20  predicted = 88\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\83\\image_01774.jpg  truth = 83  predicted = 55\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\26\\image_06526.jpg  truth = 26  predicted = 65\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\82\\image_01594.jpg  truth = 82  predicted = 91\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\51\\image_01340.jpg  truth = 51  predicted = 58\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\1\\image_06745.jpg  truth = 1  predicted = 65\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\46\\image_01038.jpg  truth = 46  predicted = 58\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\68\\image_05903.jpg  truth = 68  predicted = 75\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\52\\image_04160.jpg  truth = 52  predicted = 65\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\1\\image_06751.jpg  truth = 1  predicted = 91\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\72\\image_03593.jpg  truth = 72  predicted = 44\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\42\\image_05730.jpg  truth = 42  predicted = 65\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\1\\image_06737.jpg  truth = 1  predicted = 55\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\46\\image_01115.jpg  truth = 46  predicted = 65\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\22\\image_05391.jpg  truth = 22  predicted = 55\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\90\\image_04426.jpg  truth = 90  predicted = 55\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\1\\image_06742.jpg  truth = 1  predicted = 86\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\1\\image_06744.jpg  truth = 1  predicted = 89\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\28\\image_05214.jpg  truth = 28  predicted = 55\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\51\\image_03939.jpg  truth = 51  predicted = 91\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\82\\image_01653.jpg  truth = 82  predicted = 44\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\83\\image_01789.jpg  truth = 83  predicted = 89\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\78\\image_01830.jpg  truth = 78  predicted = 91\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\16\\image_06657.jpg  truth = 16  predicted = 74\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\15\\image_06374.jpg  truth = 15  predicted = 37\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\73\\image_00349.jpg  truth = 73  predicted = 96\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\43\\image_02371.jpg  truth = 43  predicted = 75\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\39\\image_07010.jpg  truth = 39  predicted = 3\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\51\\image_03960.jpg  truth = 51  predicted = 91\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\53\\image_03677.jpg  truth = 53  predicted = 58\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\74\\image_01200.jpg  truth = 74  predicted = 31\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\18\\image_04256.jpg  truth = 18  predicted = 56\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\51\\image_03985.jpg  truth = 51  predicted = 53\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\46\\image_00999.jpg  truth = 46  predicted = 58\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\61\\image_06266.jpg  truth = 61  predicted = 37\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\101\\image_07952.jpg  truth = 101  predicted = 58\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\94\\image_07399.jpg  truth = 94  predicted = 91\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\40\\image_04588.jpg  truth = 40  predicted = 55\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\21\\image_06805.jpg  truth = 21  predicted = 37\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\85\\image_04819.jpg  truth = 85  predicted = 43\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\46\\image_01061.jpg  truth = 46  predicted = 89\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\90\\image_04469.jpg  truth = 90  predicted = 95\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\52\\image_04240.jpg  truth = 52  predicted = 84\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\53\\image_03712.jpg  truth = 53  predicted = 65\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\71\\image_04514.jpg  truth = 71  predicted = 5\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\1\\image_06750.jpg  truth = 1  predicted = 89\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\1\\image_06735.jpg  truth = 1  predicted = 86\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\51\\image_01335.jpg  truth = 51  predicted = 4\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\25\\image_06582.jpg  truth = 25  predicted = 22\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\51\\image_03986.jpg  truth = 51  predicted = 83\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\91\\image_04875.jpg  truth = 91  predicted = 43\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\30\\image_03528.jpg  truth = 30  predicted = 86\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\1\\image_06773.jpg  truth = 1  predicted = 89\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\67\\image_07079.jpg  truth = 67  predicted = 64\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\10\\image_07090.jpg  truth = 10  predicted = 31\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\39\\image_07035.jpg  truth = 39  predicted = 78\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\52\\image_04168.jpg  truth = 52  predicted = 2\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\12\\image_04077.jpg  truth = 12  predicted = 50\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\51\\image_01468.jpg  truth = 51  predicted = 37\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\1\\image_06771.jpg  truth = 1  predicted = 65\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\46\\image_01119.jpg  truth = 46  predicted = 89\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\12\\image_04016.jpg  truth = 12  predicted = 65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\84\\image_02557.jpg  truth = 84  predicted = 37\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\83\\image_01755.jpg  truth = 83  predicted = 89\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\42\\image_05731.jpg  truth = 42  predicted = 37\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\82\\image_01609.jpg  truth = 82  predicted = 86\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\53\\image_03689.jpg  truth = 53  predicted = 81\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\90\\image_04417.jpg  truth = 90  predicted = 68\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\40\\image_04586.jpg  truth = 40  predicted = 55\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\6\\image_07185.jpg  truth = 6  predicted = 91\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\82\\image_01662.jpg  truth = 82  predicted = 44\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\27\\image_06864.jpg  truth = 27  predicted = 31\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\73\\image_00365.jpg  truth = 73  predicted = 34\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\98\\image_07758.jpg  truth = 98  predicted = 95\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\17\\image_03872.jpg  truth = 17  predicted = 34\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\93\\image_06033.jpg  truth = 93  predicted = 86\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\46\\image_01071.jpg  truth = 46  predicted = 53\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\71\\image_04555.jpg  truth = 71  predicted = 5\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\59\\image_05038.jpg  truth = 59  predicted = 37\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\4\\image_05678.jpg  truth = 4  predicted = 65\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\28\\image_05253.jpg  truth = 28  predicted = 95\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\46\\image_01092.jpg  truth = 46  predicted = 95\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\97\\image_07709.jpg  truth = 97  predicted = 3\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\85\\image_04805.jpg  truth = 85  predicted = 43\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\89\\image_00634.jpg  truth = 89  predicted = 43\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\51\\image_01316.jpg  truth = 51  predicted = 91\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\49\\image_06213.jpg  truth = 49  predicted = 34\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\100\\image_07896.jpg  truth = 100  predicted = 86\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\45\\image_07140.jpg  truth = 45  predicted = 3\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\30\\image_03484.jpg  truth = 30  predicted = 86\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\101\\image_07949.jpg  truth = 101  predicted = 58\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\77\\image_00114.jpg  truth = 77  predicted = 91\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\48\\image_04663.jpg  truth = 48  predicted = 65\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\46\\image_01026.jpg  truth = 46  predicted = 65\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\53\\image_03649.jpg  truth = 53  predicted = 58\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\59\\image_05020.jpg  truth = 59  predicted = 56\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\51\\image_03926.jpg  truth = 51  predicted = 86\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\1\\image_06759.jpg  truth = 1  predicted = 89\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\52\\image_04181.jpg  truth = 52  predicted = 65\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\83\\image_01805.jpg  truth = 83  predicted = 86\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\97\\image_07737.jpg  truth = 97  predicted = 3\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\82\\image_01603.jpg  truth = 82  predicted = 89\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\21\\image_06807.jpg  truth = 21  predicted = 37\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\11\\image_03115.jpg  truth = 11  predicted = 95\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\46\\image_00969.jpg  truth = 46  predicted = 53\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\75\\image_02111.jpg  truth = 75  predicted = 95\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\61\\image_06248.jpg  truth = 61  predicted = 95\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\53\\image_03653.jpg  truth = 53  predicted = 44\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\18\\image_04292.jpg  truth = 18  predicted = 91\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\51\\image_01362.jpg  truth = 51  predicted = 58\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\1\\image_06743.jpg  truth = 1  predicted = 51\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\1\\image_06761.jpg  truth = 1  predicted = 65\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\40\\image_04609.jpg  truth = 40  predicted = 3\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\35\\image_08088.jpg  truth = 35  predicted = 65\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\46\\image_00958.jpg  truth = 46  predicted = 58\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\51\\image_01328.jpg  truth = 51  predicted = 65\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\1\\image_06748.jpg  truth = 1  predicted = 65\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\39\\image_07018.jpg  truth = 39  predicted = 91\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\12\\image_04052.jpg  truth = 12  predicted = 65\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\53\\image_03672.jpg  truth = 53  predicted = 65\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\1\\image_06740.jpg  truth = 1  predicted = 89\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\59\\image_05064.jpg  truth = 59  predicted = 56\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\40\\image_04568.jpg  truth = 40  predicted = 96\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\90\\image_04468.jpg  truth = 90  predicted = 83\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\13\\image_05787.jpg  truth = 13  predicted = 78\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\30\\image_03481.jpg  truth = 30  predicted = 43\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\33\\image_06454.jpg  truth = 33  predicted = 65\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\11\\image_03151.jpg  truth = 11  predicted = 58\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\53\\image_03718.jpg  truth = 53  predicted = 55\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\85\\image_04825.jpg  truth = 85  predicted = 43\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\1\\image_06741.jpg  truth = 1  predicted = 86\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\85\\image_04806.jpg  truth = 85  predicted = 43\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\1\\image_06754.jpg  truth = 1  predicted = 89\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\100\\image_07926.jpg  truth = 100  predicted = 41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\94\\image_07415.jpg  truth = 94  predicted = 87\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\12\\image_03994.jpg  truth = 12  predicted = 14\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\52\\image_04221.jpg  truth = 52  predicted = 65\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\84\\image_02583.jpg  truth = 84  predicted = 87\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\85\\image_04814.jpg  truth = 85  predicted = 43\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\18\\image_04277.jpg  truth = 18  predicted = 91\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\94\\image_07436.jpg  truth = 94  predicted = 3\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\1\\image_06764.jpg  truth = 1  predicted = 55\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\1\\image_06747.jpg  truth = 1  predicted = 86\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\59\\image_05039.jpg  truth = 59  predicted = 5\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\33\\image_06479.jpg  truth = 33  predicted = 64\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\69\\image_05959.jpg  truth = 69  predicted = 78\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\29\\image_04145.jpg  truth = 29  predicted = 22\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\1\\image_06738.jpg  truth = 1  predicted = 86\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\49\\image_06202.jpg  truth = 49  predicted = 38\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\9\\image_06413.jpg  truth = 9  predicted = 3\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\100\\image_07938.jpg  truth = 100  predicted = 78\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\82\\image_01599.jpg  truth = 82  predicted = 77\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\46\\image_01109.jpg  truth = 46  predicted = 65\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\36\\image_04392.jpg  truth = 36  predicted = 91\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\73\\image_00369.jpg  truth = 73  predicted = 78\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\59\\image_05033.jpg  truth = 59  predicted = 5\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\52\\image_04169.jpg  truth = 52  predicted = 65\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\64\\image_06104.jpg  truth = 64  predicted = 65\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\92\\image_03039.jpg  truth = 92  predicted = 22\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\62\\image_08177.jpg  truth = 62  predicted = 70\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\96\\image_07606.jpg  truth = 96  predicted = 43\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\48\\image_04671.jpg  truth = 48  predicted = 65\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\74\\image_01209.jpg  truth = 74  predicted = 43\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\72\\image_03635.jpg  truth = 72  predicted = 58\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\8\\image_03314.jpg  truth = 8  predicted = 88\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\30\\image_03541.jpg  truth = 30  predicted = 80\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\51\\image_01388.jpg  truth = 51  predicted = 88\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\26\\image_06498.jpg  truth = 26  predicted = 58\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\71\\image_04515.jpg  truth = 71  predicted = 5\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\83\\image_01810.jpg  truth = 83  predicted = 55\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\9\\image_06410.jpg  truth = 9  predicted = 16\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\30\\image_03509.jpg  truth = 30  predicted = 31\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\39\\image_07043.jpg  truth = 39  predicted = 81\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\23\\image_03414.jpg  truth = 23  predicted = 67\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\46\\image_00996.jpg  truth = 46  predicted = 58\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\53\\image_03717.jpg  truth = 53  predicted = 91\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\26\\image_06500.jpg  truth = 26  predicted = 65\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\84\\image_02613.jpg  truth = 84  predicted = 37\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\94\\image_07346.jpg  truth = 94  predicted = 55\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\81\\image_00898.jpg  truth = 81  predicted = 7\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\32\\image_05614.jpg  truth = 32  predicted = 72\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\90\\image_04407.jpg  truth = 90  predicted = 89\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\90\\image_04432.jpg  truth = 90  predicted = 43\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\53\\image_03669.jpg  truth = 53  predicted = 89\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\96\\image_07676.jpg  truth = 96  predicted = 97\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\66\\image_05537.jpg  truth = 66  predicted = 34\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\5\\image_05169.jpg  truth = 5  predicted = 65\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\94\\image_07327.jpg  truth = 94  predicted = 4\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\1\\image_06734.jpg  truth = 1  predicted = 55\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\35\\image_06984.jpg  truth = 35  predicted = 38\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\76\\image_02550.jpg  truth = 76  predicted = 91\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\76\\image_02472.jpg  truth = 76  predicted = 95\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\94\\image_07358.jpg  truth = 94  predicted = 4\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\12\\image_04012.jpg  truth = 12  predicted = 65\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\57\\image_08144.jpg  truth = 57  predicted = 34\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\82\\image_01614.jpg  truth = 82  predicted = 44\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\100\\image_07936.jpg  truth = 100  predicted = 41\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\94\\image_07395.jpg  truth = 94  predicted = 78\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\1\\image_06760.jpg  truth = 1  predicted = 58\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\51\\image_03984.jpg  truth = 51  predicted = 86\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\82\\image_01679.jpg  truth = 82  predicted = 91\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\8\\image_03359.jpg  truth = 8  predicted = 65\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\28\\image_05230.jpg  truth = 28  predicted = 91\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\29\\image_04090.jpg  truth = 29  predicted = 14\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\82\\image_01661.jpg  truth = 82  predicted = 44\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\89\\image_00779.jpg  truth = 89  predicted = 58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\79\\image_06708.jpg  truth = 79  predicted = 73\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\51\\image_03954.jpg  truth = 51  predicted = 58\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\1\\image_06767.jpg  truth = 1  predicted = 51\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\26\\image_06497.jpg  truth = 26  predicted = 80\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\82\\image_01610.jpg  truth = 82  predicted = 34\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\96\\image_07683.jpg  truth = 96  predicted = 97\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\54\\image_05440.jpg  truth = 54  predicted = 41\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\46\\image_00961.jpg  truth = 46  predicted = 65\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\85\\image_04786.jpg  truth = 85  predicted = 64\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\54\\image_05430.jpg  truth = 54  predicted = 41\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\82\\image_01666.jpg  truth = 82  predicted = 55\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\92\\image_03075.jpg  truth = 92  predicted = 22\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\1\\image_06766.jpg  truth = 1  predicted = 86\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\6\\image_07199.jpg  truth = 6  predicted = 37\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\93\\image_06017.jpg  truth = 93  predicted = 38\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\46\\image_00990.jpg  truth = 46  predicted = 58\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\1\\image_06746.jpg  truth = 1  predicted = 70\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\45\\image_07139.jpg  truth = 45  predicted = 3\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\54\\image_05402.jpg  truth = 54  predicted = 41\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\93\\image_06031.jpg  truth = 93  predicted = 95\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\74\\image_01165.jpg  truth = 74  predicted = 73\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\52\\image_04167.jpg  truth = 52  predicted = 89\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\83\\image_01735.jpg  truth = 83  predicted = 44\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\51\\image_01314.jpg  truth = 51  predicted = 75\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\28\\image_05277.jpg  truth = 28  predicted = 87\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\74\\image_01305.jpg  truth = 74  predicted = 44\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\16\\image_06673.jpg  truth = 16  predicted = 65\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\66\\image_05549.jpg  truth = 66  predicted = 34\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\62\\image_07269.jpg  truth = 62  predicted = 78\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\100\\image_07902.jpg  truth = 100  predicted = 65\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\19\\image_06155.jpg  truth = 19  predicted = 34\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\1\\image_06772.jpg  truth = 1  predicted = 89\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\19\\image_06159.jpg  truth = 19  predicted = 86\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\94\\image_07388.jpg  truth = 94  predicted = 3\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\62\\image_08172.jpg  truth = 62  predicted = 91\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\61\\image_06271.jpg  truth = 61  predicted = 37\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\92\\image_03044.jpg  truth = 92  predicted = 84\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\88\\image_00563.jpg  truth = 88  predicted = 78\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\83\\image_01770.jpg  truth = 83  predicted = 86\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\30\\image_03466.jpg  truth = 30  predicted = 31\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\66\\image_05562.jpg  truth = 66  predicted = 34\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\1\\image_06753.jpg  truth = 1  predicted = 43\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\40\\image_04582.jpg  truth = 40  predicted = 44\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\46\\image_00970.jpg  truth = 46  predicted = 65\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\13\\image_05767.jpg  truth = 13  predicted = 31\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\46\\image_00976.jpg  truth = 46  predicted = 58\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\101\\image_07983.jpg  truth = 101  predicted = 58\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\18\\image_04322.jpg  truth = 18  predicted = 67\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\51\\image_03980.jpg  truth = 51  predicted = 86\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\82\\image_01618.jpg  truth = 82  predicted = 91\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\55\\image_04707.jpg  truth = 55  predicted = 91\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\51\\image_03973.jpg  truth = 51  predicted = 43\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\97\\image_07696.jpg  truth = 97  predicted = 83\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\48\\image_04627.jpg  truth = 48  predicted = 65\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\53\\image_03662.jpg  truth = 53  predicted = 91\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\94\\image_07361.jpg  truth = 94  predicted = 79\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\73\\image_00258.jpg  truth = 73  predicted = 78\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\74\\image_01213.jpg  truth = 74  predicted = 43\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\26\\image_06514.jpg  truth = 26  predicted = 89\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\28\\image_05242.jpg  truth = 28  predicted = 65\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\82\\image_01685.jpg  truth = 82  predicted = 86\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\7\\image_07218.jpg  truth = 7  predicted = 4\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\102\\image_08030.jpg  truth = 102  predicted = 65\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\51\\image_01370.jpg  truth = 51  predicted = 72\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\11\\image_03130.jpg  truth = 11  predicted = 4\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\88\\image_00482.jpg  truth = 88  predicted = 4\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\43\\image_02335.jpg  truth = 43  predicted = 75\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\28\\image_05270.jpg  truth = 28  predicted = 75\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\74\\image_01173.jpg  truth = 74  predicted = 3\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\61\\image_06249.jpg  truth = 61  predicted = 37\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\1\\image_06768.jpg  truth = 1  predicted = 97\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\82\\image_01598.jpg  truth = 82  predicted = 91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\40\\image_04587.jpg  truth = 40  predicted = 95\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\46\\image_01090.jpg  truth = 46  predicted = 65\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\7\\image_07219.jpg  truth = 7  predicted = 32\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\48\\image_04665.jpg  truth = 48  predicted = 65\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\92\\image_03049.jpg  truth = 92  predicted = 13\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\53\\image_03645.jpg  truth = 53  predicted = 65\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\34\\image_06961.jpg  truth = 34  predicted = 37\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\60\\image_02933.jpg  truth = 60  predicted = 58\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\1\\image_06757.jpg  truth = 1  predicted = 91\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\82\\image_01672.jpg  truth = 82  predicted = 44\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\90\\image_04459.jpg  truth = 90  predicted = 95\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\19\\image_06175.jpg  truth = 19  predicted = 95\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\40\\image_04573.jpg  truth = 40  predicted = 91\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\1\\image_06762.jpg  truth = 1  predicted = 91\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\75\\image_02167.jpg  truth = 75  predicted = 44\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\14\\image_06083.jpg  truth = 14  predicted = 29\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\46\\image_01078.jpg  truth = 46  predicted = 58\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\87\\image_05488.jpg  truth = 87  predicted = 2\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\84\\image_02615.jpg  truth = 84  predicted = 91\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\53\\image_03652.jpg  truth = 53  predicted = 58\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\13\\image_05769.jpg  truth = 13  predicted = 78\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\1\\image_06736.jpg  truth = 1  predicted = 89\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\30\\image_03489.jpg  truth = 30  predicted = 91\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\98\\image_07753.jpg  truth = 98  predicted = 91\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\11\\image_03176.jpg  truth = 11  predicted = 58\n",
      "C:\\jeff\\senior\\research\\text-to-image\\flower_data\\flower_data\\test\\71\\image_04512.jpg  truth = 71  predicted = 78\n",
      "Testing Loss=1.626597  Testing Accuracy=0.628842  Duration=452.16\n"
     ]
    }
   ],
   "source": [
    "test_loss_sum = 0.0\n",
    "test_correct_count = 0.0\n",
    "\n",
    "##################    \n",
    "# test the model #\\||\n",
    "##################\n",
    "testing_start_time = time.time()\n",
    "print(\"Testing...\")\n",
    "test_display = display(progress_bar(0, 100), display_id=True)\n",
    "\n",
    "model.eval()   \n",
    "cuda_is_available = False\n",
    "if cuda_is_available:\n",
    "    model.cuda()\n",
    "\n",
    "num_batches = math.ceil(len(dataloaders['test'].dataset) / batch_size)\n",
    "totalclass1 = 0\n",
    "correctlypredicted1 = 0\n",
    "totalpredicted1 = 0\n",
    "for batch_idx, (images, labels, paths) in enumerate(dataloaders['test']):\n",
    "\n",
    "    if cuda_is_available:\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        outputs = model(images)  # batch_size x 102\n",
    "        loss = criterion(outputs, labels)  # Average loss value over batch.\n",
    "\n",
    "    test_loss_sum += loss.item() * images.size(0)\n",
    "\n",
    "    _, predicted_labels = torch.max(outputs, -1)\n",
    "    test_correct_count += (predicted_labels == labels).double().sum().item()\n",
    "\n",
    "    totalclass1 += len((labels == 1).nonzero())\n",
    "    for i in (predicted_labels == 1).nonzero():\n",
    "        index = i.item()\n",
    "        if labels[index].item() == 1:\n",
    "            correctlypredicted1 += 1\n",
    "    \n",
    "    for i in range(len(labels.data)):\n",
    "        if labels[i] != predicted_labels[i]:\n",
    "            print(paths[i], \" truth =\", idx_to_class[labels[i].item()],\" predicted =\", idx_to_class[predicted_labels[i].item()])\n",
    "        if predicted_labels[i].item() == 1:\n",
    "            totalpredicted1 += 1\n",
    "        if labels[i].item() == 1:\n",
    "            totalclass1 += 1\n",
    "        if labels[i].item() == 1 and predicted_labels[i].item() == 1:\n",
    "            correctlypredicted1 += 1\n",
    "    progress = (batch_idx+1) * 100.0 / num_batches\n",
    "    test_display.update(progress_bar(progress, 100))\n",
    "\n",
    "\n",
    "testing_end_time = time.time()\n",
    "\n",
    "############################\n",
    "# calculate average losses #\n",
    "############################\n",
    "test_loss = test_loss_sum / len(dataloaders['test'].dataset)\n",
    "test_acc = test_correct_count / len(dataloaders['test'].dataset)\n",
    "\n",
    "print('Testing Loss={:.6f}  Testing Accuracy={:.6f}  Duration={:.2f}'.format(test_loss, \n",
    "                                                                             test_acc, \n",
    "                                                                             testing_end_time - testing_start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correctlypredicted1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalclass1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 100% but only 3 cases...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'37'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_class[predicted_labels[0].item()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "29 incorrect\n",
    "32 total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalpredicted1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((labels == 1).nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((predicted_labels == 1).nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len((labels == 2).nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[2].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
