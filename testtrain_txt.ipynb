{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This will train the model and also we create sample images of class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" GAN-CLS \"\"\"\n",
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "from tensorlayer.layers import *\n",
    "from tensorlayer.prepro import *\n",
    "from tensorlayer.cost import *\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.io import loadmat\n",
    "import time, os, re, nltk\n",
    "\n",
    "from utils import *\n",
    "from model import *\n",
    "import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from pickle ...\n",
      "done vocab load\n",
      "[!] samples/step1_gan-cls exists ...\n",
      "[!] samples/step_pretrain_encoder exists ...\n",
      "[!] checkpoint exists ...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data from pickle ...\")\n",
    "import pickle\n",
    "from sklearn.externals import joblib\n",
    "with open(\"_vocab.sav\", 'rb') as f:\n",
    "    vocab = joblib.load(f)\n",
    "print(\"done vocab load\")\n",
    "with open(\"_image_train.sav\", 'rb') as f:\n",
    "    _, images_train = joblib.load(f)\n",
    "with open(\"_image_test.sav\", 'rb') as f:\n",
    "    _, images_test = joblib.load(f)\n",
    "with open(\"_n.sav\", 'rb') as f:\n",
    "    n_captions_train, n_captions_test, n_captions_per_image, n_images_train, n_images_test = joblib.load(f)\n",
    "with open(\"_caption.sav\", 'rb') as f:\n",
    "    captions_ids_train, captions_ids_test = joblib.load(f)\n",
    "# images_train_256 = np.array(images_train_256)\n",
    "# images_test_256 = np.array(images_test_256)\n",
    "images_train = np.array(images_train)\n",
    "images_test = np.array(images_test)\n",
    "\n",
    "# print(n_captions_train, n_captions_test)\n",
    "# exit()\n",
    "\n",
    "ni = int(np.ceil(np.sqrt(batch_size)))\n",
    "# os.system(\"mkdir samples\")\n",
    "# os.system(\"mkdir samples/step1_gan-cls\")\n",
    "# os.system(\"mkdir checkpoint\")\n",
    "tl.files.exists_or_mkdir(\"samples/step1_gan-cls\")\n",
    "tl.files.exists_or_mkdir(\"samples/step_pretrain_encoder\")\n",
    "tl.files.exists_or_mkdir(\"checkpoint\")\n",
    "save_dir = \"checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorlayer.nlp.Vocabulary at 0x20ed16c1860>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###======================== DEFIINE MODEL ===================================###\n",
    "t_real_image = tf.placeholder('float32', [batch_size, image_size, image_size, 3], name = 'real_image')\n",
    "t_wrong_image = tf.placeholder('float32', [batch_size ,image_size, image_size, 3], name = 'wrong_image')\n",
    "t_real_caption = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name='real_caption_input')\n",
    "t_wrong_caption = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name='wrong_caption_input')\n",
    "t_z = tf.placeholder(tf.float32, [batch_size, z_dim], name='z_noise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [TL] InputLayer  cnnftxt//in: (64, 64, 64, 3)\n",
      "  [TL] Conv2dLayer cnnftxt/cnnf/h0/conv2d: shape:[4, 4, 3, 64] strides:[1, 2, 2, 1] pad:SAME act:<lambda>\n",
      "  [TL] Conv2dLayer cnnftxt/cnnf/h1/conv2d: shape:[4, 4, 64, 128] strides:[1, 2, 2, 1] pad:SAME act:identity\n",
      "  [TL] BatchNormLayer cnnftxt/cnnf/h1/batch_norm: decay:0.900000 epsilon:0.000010 act:<lambda> is_train:True\n",
      "  [TL] Conv2dLayer cnnftxt/cnnf/h2/conv2d: shape:[4, 4, 128, 256] strides:[1, 2, 2, 1] pad:SAME act:identity\n",
      "  [TL] BatchNormLayer cnnftxt/cnnf/h2/batch_norm: decay:0.900000 epsilon:0.000010 act:<lambda> is_train:True\n",
      "  [TL] Conv2dLayer cnnftxt/cnnf/h3/conv2d: shape:[4, 4, 256, 512] strides:[1, 2, 2, 1] pad:SAME act:identity\n",
      "  [TL] BatchNormLayer cnnftxt/cnnf/h3/batch_norm: decay:0.900000 epsilon:0.000010 act:<lambda> is_train:True\n",
      "  [TL] FlattenLayer cnnftxt/cnnf/h4/flatten: 8192\n",
      "  [TL] DenseLayer  cnnftxt/cnnf/h4/embed: 128 identity\n",
      "  [TL] EmbeddingInputlayer rnnftxt/rnn/wordembed: (8000, 256)\n",
      "  [TL] DynamicRNNLayer rnnftxt/rnn/dynamic: n_hidden:128, in_dim:3 in_shape:(64, ?, 256) cell_fn:BasicLSTMCell dropout:1.0 n_layer:1\n",
      "       batch_size (concurrent processes): 64\n",
      "WARNING:tensorflow:From C:\\jeff\\senior\\research\\text-to-image\\tensorlayer\\layers.py:3928: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "  [TL] InputLayer  cnnftxt//in: (64, 64, 64, 3)\n",
      "  [TL] Conv2dLayer cnnftxt/cnnf/h0/conv2d: shape:[4, 4, 3, 64] strides:[1, 2, 2, 1] pad:SAME act:<lambda>\n",
      "  [TL] Conv2dLayer cnnftxt/cnnf/h1/conv2d: shape:[4, 4, 64, 128] strides:[1, 2, 2, 1] pad:SAME act:identity\n",
      "  [TL] BatchNormLayer cnnftxt/cnnf/h1/batch_norm: decay:0.900000 epsilon:0.000010 act:<lambda> is_train:True\n",
      "  [TL] Conv2dLayer cnnftxt/cnnf/h2/conv2d: shape:[4, 4, 128, 256] strides:[1, 2, 2, 1] pad:SAME act:identity\n",
      "  [TL] BatchNormLayer cnnftxt/cnnf/h2/batch_norm: decay:0.900000 epsilon:0.000010 act:<lambda> is_train:True\n",
      "  [TL] Conv2dLayer cnnftxt/cnnf/h3/conv2d: shape:[4, 4, 256, 512] strides:[1, 2, 2, 1] pad:SAME act:identity\n",
      "  [TL] BatchNormLayer cnnftxt/cnnf/h3/batch_norm: decay:0.900000 epsilon:0.000010 act:<lambda> is_train:True\n",
      "  [TL] FlattenLayer cnnftxt/cnnf/h4/flatten: 8192\n",
      "  [TL] DenseLayer  cnnftxt/cnnf/h4/embed: 128 identity\n",
      "  [TL] EmbeddingInputlayer rnnftxt/rnn/wordembed: (8000, 256)\n",
      "  [TL] DynamicRNNLayer rnnftxt/rnn/dynamic: n_hidden:128, in_dim:3 in_shape:(64, ?, 256) cell_fn:BasicLSTMCell dropout:1.0 n_layer:1\n",
      "       batch_size (concurrent processes): 64\n"
     ]
    }
   ],
   "source": [
    "## training inference for text-to-image mapping\n",
    "net_cnn = cnn_encoder(t_real_image, is_train=True, reuse=False)\n",
    "x = net_cnn.outputs\n",
    "v = rnn_embed(t_real_caption, is_train=True, reuse=False).outputs\n",
    "x_w = cnn_encoder(t_wrong_image, is_train=True, reuse=True).outputs\n",
    "v_w = rnn_embed(t_wrong_caption, is_train=True, reuse=True).outputs\n",
    "\n",
    "alpha = 0.2 # margin alpha\n",
    "rnn_loss = tf.reduce_mean(tf.maximum(0., alpha - cosine_similarity(x, v) + cosine_similarity(x, v_w))) + \\\n",
    "            tf.reduce_mean(tf.maximum(0., alpha - cosine_similarity(x, v) + cosine_similarity(x_w, v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## training inference for txt2img\n",
    "generator_txt2img = model.generator_txt2img_resnet\n",
    "discriminator_txt2img = model.discriminator_txt2img_resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [TL] EmbeddingInputlayer rnnftxt/rnn/wordembed: (8000, 256)\n",
      "  [TL] DynamicRNNLayer rnnftxt/rnn/dynamic: n_hidden:128, in_dim:3 in_shape:(64, ?, 256) cell_fn:BasicLSTMCell dropout:None n_layer:1\n",
      "       batch_size (concurrent processes): 64\n",
      "  [TL] InputLayer  generator/g_inputz: (64, 512)\n",
      "  [TL] InputLayer  generator/g_input_txt: (64, 128)\n",
      "  [TL] DenseLayer  generator/g_reduce_text/dense: 128 <lambda>\n",
      "  [TL] ConcatLayer generator/g_concat_z_txt: 640\n",
      "  [TL] DenseLayer  generator/g_h0/dense: 16384 identity\n",
      "  [TL] BatchNormLayer generator/g_h0/batch_norm: decay:0.900000 epsilon:0.000010 act:identity is_train:True\n",
      "  [TL] ReshapeLayer generator/g_h0/reshape: (64, 4, 4, 1024)\n",
      "  [TL] Conv2dLayer generator/g_h1_res/conv2d: shape:[1, 1, 1024, 256] strides:[1, 1, 1, 1] pad:VALID act:identity\n",
      "  [TL] BatchNormLayer generator/g_h1_res/batch_norm: decay:0.900000 epsilon:0.000010 act:relu is_train:True\n",
      "  [TL] Conv2dLayer generator/g_h1_res/conv2d2: shape:[3, 3, 256, 256] strides:[1, 1, 1, 1] pad:SAME act:identity\n",
      "  [TL] BatchNormLayer generator/g_h1_res/batch_norm2: decay:0.900000 epsilon:0.000010 act:relu is_train:True\n",
      "  [TL] Conv2dLayer generator/g_h1_res/conv2d3: shape:[3, 3, 256, 1024] strides:[1, 1, 1, 1] pad:SAME act:identity\n",
      "  [TL] BatchNormLayer generator/g_h1_res/batch_norm3: decay:0.900000 epsilon:0.000010 act:identity is_train:True\n",
      "  [TL] ElementwiseLayer generator/g_h1_res/add: size:(64, 4, 4, 1024) fn:add\n",
      "  [TL] UpSampling2dLayer g_h2/upsample2d: is_scale:False size:[8, 8] method:1 align_corners:False\n",
      "  [TL] Conv2dLayer generator/g_h2/conv2d: shape:[3, 3, 1024, 512] strides:[1, 1, 1, 1] pad:SAME act:identity\n",
      "  [TL] BatchNormLayer generator/g_h2/batch_norm: decay:0.900000 epsilon:0.000010 act:identity is_train:True\n",
      "  [TL] Conv2dLayer generator/g_h3_res/conv2d: shape:[1, 1, 512, 128] strides:[1, 1, 1, 1] pad:VALID act:identity\n",
      "  [TL] BatchNormLayer generator/g_h3_res/batch_norm: decay:0.900000 epsilon:0.000010 act:relu is_train:True\n",
      "  [TL] Conv2dLayer generator/g_h3_res/conv2d2: shape:[3, 3, 128, 128] strides:[1, 1, 1, 1] pad:SAME act:identity\n",
      "  [TL] BatchNormLayer generator/g_h3_res/batch_norm2: decay:0.900000 epsilon:0.000010 act:relu is_train:True\n",
      "  [TL] Conv2dLayer generator/g_h3_res/conv2d3: shape:[3, 3, 128, 512] strides:[1, 1, 1, 1] pad:SAME act:identity\n",
      "  [TL] BatchNormLayer generator/g_h3_res/batch_norm3: decay:0.900000 epsilon:0.000010 act:identity is_train:True\n",
      "  [TL] ElementwiseLayer generator/g_h3/add: size:(64, 8, 8, 512) fn:add\n",
      "  [TL] UpSampling2dLayer g_h4/upsample2d: is_scale:False size:[16, 16] method:1 align_corners:False\n",
      "  [TL] Conv2dLayer generator/g_h4/conv2d: shape:[3, 3, 512, 256] strides:[1, 1, 1, 1] pad:SAME act:identity\n",
      "  [TL] BatchNormLayer generator/g_h4/batch_norm: decay:0.900000 epsilon:0.000010 act:relu is_train:True\n",
      "  [TL] UpSampling2dLayer g_h5/upsample2d: is_scale:False size:[32, 32] method:1 align_corners:False\n",
      "  [TL] Conv2dLayer generator/g_h5/conv2d: shape:[3, 3, 256, 128] strides:[1, 1, 1, 1] pad:SAME act:identity\n",
      "  [TL] BatchNormLayer generator/g_h5/batch_norm: decay:0.900000 epsilon:0.000010 act:relu is_train:True\n",
      "  [TL] UpSampling2dLayer g_ho/upsample2d: is_scale:False size:[64, 64] method:1 align_corners:False\n",
      "  [TL] Conv2dLayer generator/g_ho/conv2d: shape:[3, 3, 128, 3] strides:[1, 1, 1, 1] pad:SAME act:identity\n",
      "  [TL] InputLayer  discriminator/d_input/images: (64, 64, 64, 3)\n",
      "  [TL] Conv2dLayer discriminator/d_h0/conv2d: shape:[4, 4, 3, 64] strides:[1, 2, 2, 1] pad:SAME act:<lambda>\n",
      "  [TL] Conv2dLayer discriminator/d_h1/conv2d: shape:[4, 4, 64, 128] strides:[1, 2, 2, 1] pad:SAME act:identity\n",
      "  [TL] BatchNormLayer discriminator/d_h1/batchnorm: decay:0.900000 epsilon:0.000010 act:<lambda> is_train:True\n",
      "  [TL] Conv2dLayer discriminator/d_h2/conv2d: shape:[4, 4, 128, 256] strides:[1, 2, 2, 1] pad:SAME act:identity\n",
      "  [TL] BatchNormLayer discriminator/d_h2/batchnorm: decay:0.900000 epsilon:0.000010 act:<lambda> is_train:True\n",
      "  [TL] Conv2dLayer discriminator/d_h3/conv2d: shape:[4, 4, 256, 512] strides:[1, 2, 2, 1] pad:SAME act:identity\n",
      "  [TL] BatchNormLayer discriminator/d_h3/batchnorm: decay:0.900000 epsilon:0.000010 act:identity is_train:True\n",
      "  [TL] Conv2dLayer discriminator/d_h4_res/conv2d: shape:[1, 1, 512, 128] strides:[1, 1, 1, 1] pad:VALID act:identity\n",
      "  [TL] BatchNormLayer discriminator/d_h4_res/batchnorm: decay:0.900000 epsilon:0.000010 act:<lambda> is_train:True\n",
      "  [TL] Conv2dLayer discriminator/d_h4_res/conv2d2: shape:[3, 3, 128, 128] strides:[1, 1, 1, 1] pad:SAME act:identity\n",
      "  [TL] BatchNormLayer discriminator/d_h4_res/batchnorm2: decay:0.900000 epsilon:0.000010 act:<lambda> is_train:True\n",
      "  [TL] Conv2dLayer discriminator/d_h4_res/conv2d3: shape:[3, 3, 128, 512] strides:[1, 1, 1, 1] pad:SAME act:identity\n",
      "  [TL] BatchNormLayer discriminator/d_h4_res/batchnorm3: decay:0.900000 epsilon:0.000010 act:identity is_train:True\n",
      "  [TL] ElementwiseLayer discriminator/d_h4/add: size:(64, 4, 4, 512) fn:add\n",
      "  [TL] InputLayer  discriminator/d_input_txt: (64, 128)\n",
      "  [TL] DenseLayer  discriminator/d_reduce_txt/dense: 128 <lambda>\n",
      "  [TL] ExpandDimsLayer  discriminator/d_txt/expanddim1: axis:1\n",
      "  [TL] ExpandDimsLayer  discriminator/d_txt/expanddim2: axis:1\n",
      "  [TL] TileLayer  discriminator/d_txt/tile: multiples:[1, 4, 4, 1]\n",
      "  [TL] ConcatLayer discriminator/d_h3_concat: 640\n",
      "  [TL] Conv2dLayer discriminator/d_h3/conv2d_2: shape:[1, 1, 640, 512] strides:[1, 1, 1, 1] pad:VALID act:identity\n",
      "  [TL] BatchNormLayer discriminator/d_h3/batch_norm_2: decay:0.900000 epsilon:0.000010 act:<lambda> is_train:True\n",
      "  [TL] Conv2dLayer discriminator/d_ho/conv2d: shape:[4, 4, 512, 1] strides:[1, 4, 4, 1] pad:VALID act:identity\n",
      "  [TL] InputLayer  discriminator/d_input/images: (64, 64, 64, 3)\n",
      "  [TL] Conv2dLayer discriminator/d_h0/conv2d: shape:[4, 4, 3, 64] strides:[1, 2, 2, 1] pad:SAME act:<lambda>\n",
      "  [TL] Conv2dLayer discriminator/d_h1/conv2d: shape:[4, 4, 64, 128] strides:[1, 2, 2, 1] pad:SAME act:identity\n",
      "  [TL] BatchNormLayer discriminator/d_h1/batchnorm: decay:0.900000 epsilon:0.000010 act:<lambda> is_train:True\n",
      "  [TL] Conv2dLayer discriminator/d_h2/conv2d: shape:[4, 4, 128, 256] strides:[1, 2, 2, 1] pad:SAME act:identity\n",
      "  [TL] BatchNormLayer discriminator/d_h2/batchnorm: decay:0.900000 epsilon:0.000010 act:<lambda> is_train:True\n",
      "  [TL] Conv2dLayer discriminator/d_h3/conv2d: shape:[4, 4, 256, 512] strides:[1, 2, 2, 1] pad:SAME act:identity\n",
      "  [TL] BatchNormLayer discriminator/d_h3/batchnorm: decay:0.900000 epsilon:0.000010 act:identity is_train:True\n",
      "  [TL] Conv2dLayer discriminator/d_h4_res/conv2d: shape:[1, 1, 512, 128] strides:[1, 1, 1, 1] pad:VALID act:identity\n",
      "  [TL] BatchNormLayer discriminator/d_h4_res/batchnorm: decay:0.900000 epsilon:0.000010 act:<lambda> is_train:True\n",
      "  [TL] Conv2dLayer discriminator/d_h4_res/conv2d2: shape:[3, 3, 128, 128] strides:[1, 1, 1, 1] pad:SAME act:identity\n",
      "  [TL] BatchNormLayer discriminator/d_h4_res/batchnorm2: decay:0.900000 epsilon:0.000010 act:<lambda> is_train:True\n",
      "  [TL] Conv2dLayer discriminator/d_h4_res/conv2d3: shape:[3, 3, 128, 512] strides:[1, 1, 1, 1] pad:SAME act:identity\n",
      "  [TL] BatchNormLayer discriminator/d_h4_res/batchnorm3: decay:0.900000 epsilon:0.000010 act:identity is_train:True\n",
      "  [TL] ElementwiseLayer discriminator/d_h4/add: size:(64, 4, 4, 512) fn:add\n",
      "  [TL] InputLayer  discriminator/d_input_txt: (64, 128)\n",
      "  [TL] DenseLayer  discriminator/d_reduce_txt/dense: 128 <lambda>\n",
      "  [TL] ExpandDimsLayer  discriminator/d_txt/expanddim1: axis:1\n",
      "  [TL] ExpandDimsLayer  discriminator/d_txt/expanddim2: axis:1\n",
      "  [TL] TileLayer  discriminator/d_txt/tile: multiples:[1, 4, 4, 1]\n",
      "  [TL] ConcatLayer discriminator/d_h3_concat: 640\n",
      "  [TL] Conv2dLayer discriminator/d_h3/conv2d_2: shape:[1, 1, 640, 512] strides:[1, 1, 1, 1] pad:VALID act:identity\n",
      "  [TL] BatchNormLayer discriminator/d_h3/batch_norm_2: decay:0.900000 epsilon:0.000010 act:<lambda> is_train:True\n",
      "  [TL] Conv2dLayer discriminator/d_ho/conv2d: shape:[4, 4, 512, 1] strides:[1, 4, 4, 1] pad:VALID act:identity\n",
      "  [TL] EmbeddingInputlayer rnnftxt/rnn/wordembed: (8000, 256)\n",
      "  [TL] DynamicRNNLayer rnnftxt/rnn/dynamic: n_hidden:128, in_dim:3 in_shape:(64, ?, 256) cell_fn:BasicLSTMCell dropout:None n_layer:1\n",
      "       batch_size (concurrent processes): 64\n",
      "  [TL] InputLayer  discriminator/d_input/images: (64, 64, 64, 3)\n",
      "  [TL] Conv2dLayer discriminator/d_h0/conv2d: shape:[4, 4, 3, 64] strides:[1, 2, 2, 1] pad:SAME act:<lambda>\n",
      "  [TL] Conv2dLayer discriminator/d_h1/conv2d: shape:[4, 4, 64, 128] strides:[1, 2, 2, 1] pad:SAME act:identity\n",
      "  [TL] BatchNormLayer discriminator/d_h1/batchnorm: decay:0.900000 epsilon:0.000010 act:<lambda> is_train:True\n",
      "  [TL] Conv2dLayer discriminator/d_h2/conv2d: shape:[4, 4, 128, 256] strides:[1, 2, 2, 1] pad:SAME act:identity\n",
      "  [TL] BatchNormLayer discriminator/d_h2/batchnorm: decay:0.900000 epsilon:0.000010 act:<lambda> is_train:True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [TL] Conv2dLayer discriminator/d_h3/conv2d: shape:[4, 4, 256, 512] strides:[1, 2, 2, 1] pad:SAME act:identity\n",
      "  [TL] BatchNormLayer discriminator/d_h3/batchnorm: decay:0.900000 epsilon:0.000010 act:identity is_train:True\n",
      "  [TL] Conv2dLayer discriminator/d_h4_res/conv2d: shape:[1, 1, 512, 128] strides:[1, 1, 1, 1] pad:VALID act:identity\n",
      "  [TL] BatchNormLayer discriminator/d_h4_res/batchnorm: decay:0.900000 epsilon:0.000010 act:<lambda> is_train:True\n",
      "  [TL] Conv2dLayer discriminator/d_h4_res/conv2d2: shape:[3, 3, 128, 128] strides:[1, 1, 1, 1] pad:SAME act:identity\n",
      "  [TL] BatchNormLayer discriminator/d_h4_res/batchnorm2: decay:0.900000 epsilon:0.000010 act:<lambda> is_train:True\n",
      "  [TL] Conv2dLayer discriminator/d_h4_res/conv2d3: shape:[3, 3, 128, 512] strides:[1, 1, 1, 1] pad:SAME act:identity\n",
      "  [TL] BatchNormLayer discriminator/d_h4_res/batchnorm3: decay:0.900000 epsilon:0.000010 act:identity is_train:True\n",
      "  [TL] ElementwiseLayer discriminator/d_h4/add: size:(64, 4, 4, 512) fn:add\n",
      "  [TL] InputLayer  discriminator/d_input_txt: (64, 128)\n",
      "  [TL] DenseLayer  discriminator/d_reduce_txt/dense: 128 <lambda>\n",
      "  [TL] ExpandDimsLayer  discriminator/d_txt/expanddim1: axis:1\n",
      "  [TL] ExpandDimsLayer  discriminator/d_txt/expanddim2: axis:1\n",
      "  [TL] TileLayer  discriminator/d_txt/tile: multiples:[1, 4, 4, 1]\n",
      "  [TL] ConcatLayer discriminator/d_h3_concat: 640\n",
      "  [TL] Conv2dLayer discriminator/d_h3/conv2d_2: shape:[1, 1, 640, 512] strides:[1, 1, 1, 1] pad:VALID act:identity\n",
      "  [TL] BatchNormLayer discriminator/d_h3/batch_norm_2: decay:0.900000 epsilon:0.000010 act:<lambda> is_train:True\n",
      "  [TL] Conv2dLayer discriminator/d_ho/conv2d: shape:[4, 4, 512, 1] strides:[1, 4, 4, 1] pad:VALID act:identity\n",
      "  [TL] EmbeddingInputlayer rnnftxt/rnn/wordembed: (8000, 256)\n",
      "  [TL] DynamicRNNLayer rnnftxt/rnn/dynamic: n_hidden:128, in_dim:3 in_shape:(64, ?, 256) cell_fn:BasicLSTMCell dropout:None n_layer:1\n",
      "       batch_size (concurrent processes): 64\n",
      "  [TL] InputLayer  generator/g_inputz: (64, 512)\n",
      "  [TL] InputLayer  generator/g_input_txt: (64, 128)\n",
      "  [TL] DenseLayer  generator/g_reduce_text/dense: 128 <lambda>\n",
      "  [TL] ConcatLayer generator/g_concat_z_txt: 640\n",
      "  [TL] DenseLayer  generator/g_h0/dense: 16384 identity\n",
      "  [TL] BatchNormLayer generator/g_h0/batch_norm: decay:0.900000 epsilon:0.000010 act:identity is_train:False\n",
      "  [TL] ReshapeLayer generator/g_h0/reshape: (64, 4, 4, 1024)\n",
      "  [TL] Conv2dLayer generator/g_h1_res/conv2d: shape:[1, 1, 1024, 256] strides:[1, 1, 1, 1] pad:VALID act:identity\n",
      "  [TL] BatchNormLayer generator/g_h1_res/batch_norm: decay:0.900000 epsilon:0.000010 act:relu is_train:False\n",
      "  [TL] Conv2dLayer generator/g_h1_res/conv2d2: shape:[3, 3, 256, 256] strides:[1, 1, 1, 1] pad:SAME act:identity\n",
      "  [TL] BatchNormLayer generator/g_h1_res/batch_norm2: decay:0.900000 epsilon:0.000010 act:relu is_train:False\n",
      "  [TL] Conv2dLayer generator/g_h1_res/conv2d3: shape:[3, 3, 256, 1024] strides:[1, 1, 1, 1] pad:SAME act:identity\n",
      "  [TL] BatchNormLayer generator/g_h1_res/batch_norm3: decay:0.900000 epsilon:0.000010 act:identity is_train:False\n",
      "  [TL] ElementwiseLayer generator/g_h1_res/add: size:(64, 4, 4, 1024) fn:add\n",
      "  [TL] UpSampling2dLayer g_h2/upsample2d: is_scale:False size:[8, 8] method:1 align_corners:False\n",
      "  [TL] Conv2dLayer generator/g_h2/conv2d: shape:[3, 3, 1024, 512] strides:[1, 1, 1, 1] pad:SAME act:identity\n",
      "  [TL] BatchNormLayer generator/g_h2/batch_norm: decay:0.900000 epsilon:0.000010 act:identity is_train:False\n",
      "  [TL] Conv2dLayer generator/g_h3_res/conv2d: shape:[1, 1, 512, 128] strides:[1, 1, 1, 1] pad:VALID act:identity\n",
      "  [TL] BatchNormLayer generator/g_h3_res/batch_norm: decay:0.900000 epsilon:0.000010 act:relu is_train:False\n",
      "  [TL] Conv2dLayer generator/g_h3_res/conv2d2: shape:[3, 3, 128, 128] strides:[1, 1, 1, 1] pad:SAME act:identity\n",
      "  [TL] BatchNormLayer generator/g_h3_res/batch_norm2: decay:0.900000 epsilon:0.000010 act:relu is_train:False\n",
      "  [TL] Conv2dLayer generator/g_h3_res/conv2d3: shape:[3, 3, 128, 512] strides:[1, 1, 1, 1] pad:SAME act:identity\n",
      "  [TL] BatchNormLayer generator/g_h3_res/batch_norm3: decay:0.900000 epsilon:0.000010 act:identity is_train:False\n",
      "  [TL] ElementwiseLayer generator/g_h3/add: size:(64, 8, 8, 512) fn:add\n",
      "  [TL] UpSampling2dLayer g_h4/upsample2d: is_scale:False size:[16, 16] method:1 align_corners:False\n",
      "  [TL] Conv2dLayer generator/g_h4/conv2d: shape:[3, 3, 512, 256] strides:[1, 1, 1, 1] pad:SAME act:identity\n",
      "  [TL] BatchNormLayer generator/g_h4/batch_norm: decay:0.900000 epsilon:0.000010 act:relu is_train:False\n",
      "  [TL] UpSampling2dLayer g_h5/upsample2d: is_scale:False size:[32, 32] method:1 align_corners:False\n",
      "  [TL] Conv2dLayer generator/g_h5/conv2d: shape:[3, 3, 256, 128] strides:[1, 1, 1, 1] pad:SAME act:identity\n",
      "  [TL] BatchNormLayer generator/g_h5/batch_norm: decay:0.900000 epsilon:0.000010 act:relu is_train:False\n",
      "  [TL] UpSampling2dLayer g_ho/upsample2d: is_scale:False size:[64, 64] method:1 align_corners:False\n",
      "  [TL] Conv2dLayer generator/g_ho/conv2d: shape:[3, 3, 128, 3] strides:[1, 1, 1, 1] pad:SAME act:identity\n"
     ]
    }
   ],
   "source": [
    "net_rnn = rnn_embed(t_real_caption, is_train=False, reuse=True)\n",
    "net_fake_image, _ = generator_txt2img(t_z,\n",
    "                net_rnn.outputs,\n",
    "                is_train=True, reuse=False, batch_size=batch_size)\n",
    "                #+ tf.random_normal(shape=net_rnn.outputs.get_shape(), mean=0, stddev=0.02), # NOISE ON RNN\n",
    "net_d, disc_fake_image_logits = discriminator_txt2img(\n",
    "                net_fake_image.outputs, net_rnn.outputs, is_train=True, reuse=False)\n",
    "_, disc_real_image_logits = discriminator_txt2img(\n",
    "                t_real_image, net_rnn.outputs, is_train=True, reuse=True)\n",
    "_, disc_mismatch_logits = discriminator_txt2img(\n",
    "                # t_wrong_image,\n",
    "                t_real_image,\n",
    "                # net_rnn.outputs,\n",
    "                rnn_embed(t_wrong_caption, is_train=False, reuse=True).outputs,\n",
    "                is_train=True, reuse=True)\n",
    "\n",
    "## testing inference for txt2img\n",
    "net_g, _ = generator_txt2img(t_z,\n",
    "                rnn_embed(t_real_caption, is_train=False, reuse=True).outputs,\n",
    "                is_train=False, reuse=True, batch_size=batch_size)\n",
    "\n",
    "d_loss1 = tl.cost.sigmoid_cross_entropy(disc_real_image_logits, tf.ones_like(disc_real_image_logits), name='d1')\n",
    "d_loss2 = tl.cost.sigmoid_cross_entropy(disc_mismatch_logits,  tf.zeros_like(disc_mismatch_logits), name='d2')\n",
    "d_loss3 = tl.cost.sigmoid_cross_entropy(disc_fake_image_logits, tf.zeros_like(disc_fake_image_logits), name='d3')\n",
    "d_loss = d_loss1 + (d_loss2 + d_loss3) * 0.5\n",
    "g_loss = tl.cost.sigmoid_cross_entropy(disc_fake_image_logits, tf.ones_like(disc_fake_image_logits), name='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [*] geting variables with cnn\n",
      "  got   0: cnnftxt/cnnf/h0/conv2d/W_conv2d:0   (4, 4, 3, 64)\n",
      "  got   1: cnnftxt/cnnf/h0/conv2d/b_conv2d:0   (64,)\n",
      "  got   2: cnnftxt/cnnf/h1/conv2d/W_conv2d:0   (4, 4, 64, 128)\n",
      "  got   3: cnnftxt/cnnf/h1/batch_norm/beta:0   (128,)\n",
      "  got   4: cnnftxt/cnnf/h1/batch_norm/gamma:0   (128,)\n",
      "  got   5: cnnftxt/cnnf/h2/conv2d/W_conv2d:0   (4, 4, 128, 256)\n",
      "  got   6: cnnftxt/cnnf/h2/batch_norm/beta:0   (256,)\n",
      "  got   7: cnnftxt/cnnf/h2/batch_norm/gamma:0   (256,)\n",
      "  got   8: cnnftxt/cnnf/h3/conv2d/W_conv2d:0   (4, 4, 256, 512)\n",
      "  got   9: cnnftxt/cnnf/h3/batch_norm/beta:0   (512,)\n",
      "  got  10: cnnftxt/cnnf/h3/batch_norm/gamma:0   (512,)\n",
      "  got  11: cnnftxt/cnnf/h4/embed/W:0   (8192, 128)\n",
      "  [*] geting variables with rnn\n",
      "  got   0: rnnftxt/rnn/wordembed/embeddings:0   (8000, 256)\n",
      "  got   1: rnnftxt/rnn/dynamic/rnn/basic_lstm_cell/kernel:0   (384, 512)\n",
      "  got   2: rnnftxt/rnn/dynamic/rnn/basic_lstm_cell/bias:0   (512,)\n",
      "  [*] geting variables with discriminator\n",
      "  got   0: discriminator/d_h0/conv2d/W_conv2d:0   (4, 4, 3, 64)\n",
      "  got   1: discriminator/d_h0/conv2d/b_conv2d:0   (64,)\n",
      "  got   2: discriminator/d_h1/conv2d/W_conv2d:0   (4, 4, 64, 128)\n",
      "  got   3: discriminator/d_h1/batchnorm/beta:0   (128,)\n",
      "  got   4: discriminator/d_h1/batchnorm/gamma:0   (128,)\n",
      "  got   5: discriminator/d_h2/conv2d/W_conv2d:0   (4, 4, 128, 256)\n",
      "  got   6: discriminator/d_h2/batchnorm/beta:0   (256,)\n",
      "  got   7: discriminator/d_h2/batchnorm/gamma:0   (256,)\n",
      "  got   8: discriminator/d_h3/conv2d/W_conv2d:0   (4, 4, 256, 512)\n",
      "  got   9: discriminator/d_h3/batchnorm/beta:0   (512,)\n",
      "  got  10: discriminator/d_h3/batchnorm/gamma:0   (512,)\n",
      "  got  11: discriminator/d_h4_res/conv2d/W_conv2d:0   (1, 1, 512, 128)\n",
      "  got  12: discriminator/d_h4_res/batchnorm/beta:0   (128,)\n",
      "  got  13: discriminator/d_h4_res/batchnorm/gamma:0   (128,)\n",
      "  got  14: discriminator/d_h4_res/conv2d2/W_conv2d:0   (3, 3, 128, 128)\n",
      "  got  15: discriminator/d_h4_res/batchnorm2/beta:0   (128,)\n",
      "  got  16: discriminator/d_h4_res/batchnorm2/gamma:0   (128,)\n",
      "  got  17: discriminator/d_h4_res/conv2d3/W_conv2d:0   (3, 3, 128, 512)\n",
      "  got  18: discriminator/d_h4_res/batchnorm3/beta:0   (512,)\n",
      "  got  19: discriminator/d_h4_res/batchnorm3/gamma:0   (512,)\n",
      "  got  20: discriminator/d_reduce_txt/dense/W:0   (128, 128)\n",
      "  got  21: discriminator/d_reduce_txt/dense/b:0   (128,)\n",
      "  got  22: discriminator/d_h3/conv2d_2/W_conv2d:0   (1, 1, 640, 512)\n",
      "  got  23: discriminator/d_h3/batch_norm_2/beta:0   (512,)\n",
      "  got  24: discriminator/d_h3/batch_norm_2/gamma:0   (512,)\n",
      "  got  25: discriminator/d_ho/conv2d/W_conv2d:0   (4, 4, 512, 1)\n",
      "  got  26: discriminator/d_ho/conv2d/b_conv2d:0   (1,)\n",
      "  [*] geting variables with generator\n",
      "  got   0: generator/g_reduce_text/dense/W:0   (128, 128)\n",
      "  got   1: generator/g_reduce_text/dense/b:0   (128,)\n",
      "  got   2: generator/g_h0/dense/W:0   (640, 16384)\n",
      "  got   3: generator/g_h0/batch_norm/beta:0   (16384,)\n",
      "  got   4: generator/g_h0/batch_norm/gamma:0   (16384,)\n",
      "  got   5: generator/g_h1_res/conv2d/W_conv2d:0   (1, 1, 1024, 256)\n",
      "  got   6: generator/g_h1_res/batch_norm/beta:0   (256,)\n",
      "  got   7: generator/g_h1_res/batch_norm/gamma:0   (256,)\n",
      "  got   8: generator/g_h1_res/conv2d2/W_conv2d:0   (3, 3, 256, 256)\n",
      "  got   9: generator/g_h1_res/batch_norm2/beta:0   (256,)\n",
      "  got  10: generator/g_h1_res/batch_norm2/gamma:0   (256,)\n",
      "  got  11: generator/g_h1_res/conv2d3/W_conv2d:0   (3, 3, 256, 1024)\n",
      "  got  12: generator/g_h1_res/batch_norm3/beta:0   (1024,)\n",
      "  got  13: generator/g_h1_res/batch_norm3/gamma:0   (1024,)\n",
      "  got  14: generator/g_h2/conv2d/W_conv2d:0   (3, 3, 1024, 512)\n",
      "  got  15: generator/g_h2/batch_norm/beta:0   (512,)\n",
      "  got  16: generator/g_h2/batch_norm/gamma:0   (512,)\n",
      "  got  17: generator/g_h3_res/conv2d/W_conv2d:0   (1, 1, 512, 128)\n",
      "  got  18: generator/g_h3_res/batch_norm/beta:0   (128,)\n",
      "  got  19: generator/g_h3_res/batch_norm/gamma:0   (128,)\n",
      "  got  20: generator/g_h3_res/conv2d2/W_conv2d:0   (3, 3, 128, 128)\n",
      "  got  21: generator/g_h3_res/batch_norm2/beta:0   (128,)\n",
      "  got  22: generator/g_h3_res/batch_norm2/gamma:0   (128,)\n",
      "  got  23: generator/g_h3_res/conv2d3/W_conv2d:0   (3, 3, 128, 512)\n",
      "  got  24: generator/g_h3_res/batch_norm3/beta:0   (512,)\n",
      "  got  25: generator/g_h3_res/batch_norm3/gamma:0   (512,)\n",
      "  got  26: generator/g_h4/conv2d/W_conv2d:0   (3, 3, 512, 256)\n",
      "  got  27: generator/g_h4/batch_norm/beta:0   (256,)\n",
      "  got  28: generator/g_h4/batch_norm/gamma:0   (256,)\n",
      "  got  29: generator/g_h5/conv2d/W_conv2d:0   (3, 3, 256, 128)\n",
      "  got  30: generator/g_h5/batch_norm/beta:0   (128,)\n",
      "  got  31: generator/g_h5/batch_norm/gamma:0   (128,)\n",
      "  got  32: generator/g_ho/conv2d/W_conv2d:0   (3, 3, 128, 3)\n",
      "  got  33: generator/g_ho/conv2d/b_conv2d:0   (3,)\n"
     ]
    }
   ],
   "source": [
    "####======================== DEFINE TRAIN OPTS ==============================###\n",
    "lr = 0.0002\n",
    "lr_decay = 0.5      # decay factor for adam, https://github.com/reedscot/icml2016/blob/master/main_cls_int.lua  https://github.com/reedscot/icml2016/blob/master/scripts/train_flowers.sh\n",
    "decay_every = 100   # https://github.com/reedscot/icml2016/blob/master/main_cls.lua\n",
    "beta1 = 0.5\n",
    "\n",
    "cnn_vars = tl.layers.get_variables_with_name('cnn', True, True)\n",
    "rnn_vars = tl.layers.get_variables_with_name('rnn', True, True)\n",
    "d_vars = tl.layers.get_variables_with_name('discriminator', True, True)\n",
    "g_vars = tl.layers.get_variables_with_name('generator', True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cseuser\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('learning_rate'):\n",
    "    lr_v = tf.Variable(lr, trainable=False)\n",
    "d_optim = tf.train.AdamOptimizer(lr_v, beta1=beta1).minimize(d_loss, var_list=d_vars )\n",
    "g_optim = tf.train.AdamOptimizer(lr_v, beta1=beta1).minimize(g_loss, var_list=g_vars )\n",
    "# e_optim = tf.train.AdamOptimizer(lr_v, beta1=beta1).minimize(e_loss, var_list=e_vars + c_vars)\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(rnn_loss, rnn_vars + cnn_vars), 10)\n",
    "optimizer = tf.train.AdamOptimizer(lr_v, beta1=beta1)# optimizer = tf.train.GradientDescentOptimizer(lre)\n",
    "rnn_optim = optimizer.apply_gradients(zip(grads, rnn_vars + cnn_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Loading checkpoint\\net_rnn.npz model SUCCESS!\n",
      "[*] Loading checkpoint\\net_cnn.npz model SUCCESS!\n",
      "[*] Loading checkpoint\\net_g.npz model SUCCESS!\n",
      "[*] Loading checkpoint\\net_d.npz model SUCCESS!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorlayer.layers.Conv2dLayer at 0x21039f1a0f0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adam_vars = tl.layers.get_variables_with_name('Adam', False, True)\n",
    "\n",
    "###============================ TRAINING ====================================###\n",
    "sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
    "tl.layers.initialize_global_variables(sess)\n",
    "\n",
    "# load the latest checkpoints\n",
    "net_rnn_name = os.path.join(save_dir, 'net_rnn.npz')\n",
    "net_cnn_name = os.path.join(save_dir, 'net_cnn.npz')\n",
    "net_g_name = os.path.join(save_dir, 'net_g.npz')\n",
    "net_d_name = os.path.join(save_dir, 'net_d.npz')\n",
    "\n",
    "load_and_assign_npz(sess=sess, name=net_rnn_name, model=net_rnn)\n",
    "load_and_assign_npz(sess=sess, name=net_cnn_name, model=net_cnn)\n",
    "load_and_assign_npz(sess=sess, name=net_g_name, model=net_g)\n",
    "load_and_assign_npz(sess=sess, name=net_d_name, model=net_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## seed for generation, z and sentence ids\n",
    "## batch+size is 64 from models.py\n",
    "sample_size = batch_size\n",
    "sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, z_dim)).astype(np.float32)\n",
    "    # sample_seed = np.random.uniform(low=-1, high=1, size=(sample_size, z_dim)).astype(np.float32)]\n",
    "n = int(sample_size / ni)\n",
    "sample_sentence = [\"the flower shown has yellow anther red pistil and bright red petals.\"] * n + \\\n",
    "                  [\"this flower has petals that are yellow, white and purple and has dark lines\"] * n + \\\n",
    "                  [\"the petals on this flower are white with a yellow center\"] * n + \\\n",
    "                  [\"this flower has a lot of small round pink petals.\"] * n + \\\n",
    "                  [\"this flower is orange in color, and has petals that are ruffled and rounded.\"] * n + \\\n",
    "                  [\"the flower has yellow petals and the center of it is brown.\"] * n + \\\n",
    "                  [\"this flower has petals that are blue and white.\"] * n + \\\n",
    "                  [\"this flower is orange and has pink dots\"] * n\n",
    "\n",
    "#[\"these white flowers have petals that start off white in color and end in a white towards the tips.\"] * n + \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = [\"this flower is orange and has pink dots\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-c3446a6ee59f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_to_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend_id\u001b[0m\u001b[1;33m]\u001b[0m    \u001b[1;31m# add END_ID\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sentence' is not defined"
     ]
    }
   ],
   "source": [
    "test[0] = [vocab.word_to_id(word) for word in nltk.tokenize.word_tokenize(sentence)] + [vocab.end_id]    # add END_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'this flower is orange and has pink dots'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-4110d591864a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepro\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"post\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\jeff\\senior\\research\\text-to-image\\tensorlayer\\prepro.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[1;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[0;32m   1268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1269\u001b[0m         \u001b[1;31m# check `trunc` has expected shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1270\u001b[1;33m         \u001b[0mtrunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1271\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtrunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1272\u001b[0m             raise ValueError('Shape of sample %s of sequence at position %s is different from expected shape %s' %\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m     \"\"\"\n\u001b[1;32m--> 538\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'this flower is orange and has pink dots'"
     ]
    }
   ],
   "source": [
    "test = tl.prepro.pad_sequences(test, padding = \"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: the flower shown has yellow anther red pistil and bright red petals.\n",
      "seed: the flower shown has yellow anther red pistil and bright red petals.\n",
      "seed: the flower shown has yellow anther red pistil and bright red petals.\n",
      "seed: the flower shown has yellow anther red pistil and bright red petals.\n",
      "seed: the flower shown has yellow anther red pistil and bright red petals.\n",
      "seed: the flower shown has yellow anther red pistil and bright red petals.\n",
      "seed: the flower shown has yellow anther red pistil and bright red petals.\n",
      "seed: the flower shown has yellow anther red pistil and bright red petals.\n",
      "seed: this flower has petals that are yellow, white and purple and has dark lines\n",
      "seed: this flower has petals that are yellow, white and purple and has dark lines\n",
      "seed: this flower has petals that are yellow, white and purple and has dark lines\n",
      "seed: this flower has petals that are yellow, white and purple and has dark lines\n",
      "seed: this flower has petals that are yellow, white and purple and has dark lines\n",
      "seed: this flower has petals that are yellow, white and purple and has dark lines\n",
      "seed: this flower has petals that are yellow, white and purple and has dark lines\n",
      "seed: this flower has petals that are yellow, white and purple and has dark lines\n",
      "seed: the petals on this flower are white with a yellow center\n",
      "seed: the petals on this flower are white with a yellow center\n",
      "seed: the petals on this flower are white with a yellow center\n",
      "seed: the petals on this flower are white with a yellow center\n",
      "seed: the petals on this flower are white with a yellow center\n",
      "seed: the petals on this flower are white with a yellow center\n",
      "seed: the petals on this flower are white with a yellow center\n",
      "seed: the petals on this flower are white with a yellow center\n",
      "seed: this flower has a lot of small round pink petals.\n",
      "seed: this flower has a lot of small round pink petals.\n",
      "seed: this flower has a lot of small round pink petals.\n",
      "seed: this flower has a lot of small round pink petals.\n",
      "seed: this flower has a lot of small round pink petals.\n",
      "seed: this flower has a lot of small round pink petals.\n",
      "seed: this flower has a lot of small round pink petals.\n",
      "seed: this flower has a lot of small round pink petals.\n",
      "seed: this flower is orange in color, and has petals that are ruffled and rounded.\n",
      "seed: this flower is orange in color, and has petals that are ruffled and rounded.\n",
      "seed: this flower is orange in color, and has petals that are ruffled and rounded.\n",
      "seed: this flower is orange in color, and has petals that are ruffled and rounded.\n",
      "seed: this flower is orange in color, and has petals that are ruffled and rounded.\n",
      "seed: this flower is orange in color, and has petals that are ruffled and rounded.\n",
      "seed: this flower is orange in color, and has petals that are ruffled and rounded.\n",
      "seed: this flower is orange in color, and has petals that are ruffled and rounded.\n",
      "seed: the flower has yellow petals and the center of it is brown.\n",
      "seed: the flower has yellow petals and the center of it is brown.\n",
      "seed: the flower has yellow petals and the center of it is brown.\n",
      "seed: the flower has yellow petals and the center of it is brown.\n",
      "seed: the flower has yellow petals and the center of it is brown.\n",
      "seed: the flower has yellow petals and the center of it is brown.\n",
      "seed: the flower has yellow petals and the center of it is brown.\n",
      "seed: the flower has yellow petals and the center of it is brown.\n",
      "seed: this flower has petals that are blue and white.\n",
      "seed: this flower has petals that are blue and white.\n",
      "seed: this flower has petals that are blue and white.\n",
      "seed: this flower has petals that are blue and white.\n",
      "seed: this flower has petals that are blue and white.\n",
      "seed: this flower has petals that are blue and white.\n",
      "seed: this flower has petals that are blue and white.\n",
      "seed: this flower has petals that are blue and white.\n",
      "seed: this flower is orange and has pink dots\n",
      "seed: this flower is orange and has pink dots\n",
      "seed: this flower is orange and has pink dots\n",
      "seed: this flower is orange and has pink dots\n",
      "seed: this flower is orange and has pink dots\n",
      "seed: this flower is orange and has pink dots\n",
      "seed: this flower is orange and has pink dots\n",
      "seed: this flower is orange and has pink dots\n"
     ]
    }
   ],
   "source": [
    "# sample_sentence = captions_ids_test[0:sample_size]\n",
    "# tokenize and add ids to the words. Also add a 2 (vocab.end_id) and some 0's from the prepro\n",
    "for i, sentence in enumerate(sample_sentence):\n",
    "    print(\"seed: %s\" % sentence)\n",
    "    sentence = preprocess_caption(sentence)\n",
    "    sample_sentence[i] = [vocab.word_to_id(word) for word in nltk.tokenize.word_tokenize(sentence)] + [vocab.end_id]    # add END_ID\n",
    "    # sample_sentence[i] = [vocab.word_to_id(word) for word in sentence]\n",
    "    # print(sample_sentence[i])\n",
    "sample_sentence = tl.prepro.pad_sequences(sample_sentence, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  6,   3,  17,  24,   5,   7,  16, 105,   2,   0,   0,   0,   0,\n",
       "         0,   0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sentence[60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "img_dir = os.path.join(cwd, '102flowers')\n",
    "class1_dir = os.path.join(cwd, 'text_c10\\\\class_00001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alllines = []\n",
    "for filename in os.listdir(class1_dir):\n",
    "    \n",
    "    if filename.endswith(\".txt\"):\n",
    "        filepath = os.path.join(class1_dir, filename)\n",
    "        text_file = open(filepath, \"r\")\n",
    "        lines = text_file.read().split('\\n')\n",
    "        for a in lines:\n",
    "            if a == \"\":\n",
    "                continue\n",
    "            alllines.append(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This combines all the text for the class 1 images and saves it to alllines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import randrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(randrange(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below we run class1 text on the gan and save the output to class00001images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving class00001images/0.png\n",
      "saving class00001images/1.png\n",
      "saving class00001images/2.png\n",
      "saving class00001images/3.png\n",
      "saving class00001images/4.png\n",
      "saving class00001images/5.png\n",
      "saving class00001images/6.png\n",
      "saving class00001images/7.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cseuser\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving class00001images/8.png\n",
      "saving class00001images/9.png\n",
      "saving class00001images/10.png\n",
      "saving class00001images/11.png\n",
      "saving class00001images/12.png\n",
      "saving class00001images/13.png\n",
      "saving class00001images/14.png\n",
      "saving class00001images/15.png\n",
      "saving class00001images/16.png\n",
      "saving class00001images/17.png\n",
      "saving class00001images/18.png\n",
      "saving class00001images/19.png\n",
      "saving class00001images/20.png\n",
      "saving class00001images/21.png\n",
      "saving class00001images/22.png\n",
      "saving class00001images/23.png\n",
      "saving class00001images/24.png\n",
      "saving class00001images/25.png\n",
      "saving class00001images/26.png\n",
      "saving class00001images/27.png\n",
      "saving class00001images/28.png\n",
      "saving class00001images/29.png\n",
      "saving class00001images/30.png\n",
      "saving class00001images/31.png\n",
      "saving class00001images/32.png\n",
      "saving class00001images/33.png\n",
      "saving class00001images/34.png\n",
      "saving class00001images/35.png\n",
      "saving class00001images/36.png\n",
      "saving class00001images/37.png\n",
      "saving class00001images/38.png\n",
      "saving class00001images/39.png\n",
      "saving class00001images/40.png\n",
      "saving class00001images/41.png\n",
      "saving class00001images/42.png\n",
      "saving class00001images/43.png\n",
      "saving class00001images/44.png\n",
      "saving class00001images/45.png\n",
      "saving class00001images/46.png\n",
      "saving class00001images/47.png\n",
      "saving class00001images/48.png\n",
      "saving class00001images/49.png\n",
      "saving class00001images/50.png\n",
      "saving class00001images/51.png\n",
      "saving class00001images/52.png\n",
      "saving class00001images/53.png\n",
      "saving class00001images/54.png\n",
      "saving class00001images/55.png\n",
      "saving class00001images/56.png\n",
      "saving class00001images/57.png\n",
      "saving class00001images/58.png\n",
      "saving class00001images/59.png\n",
      "saving class00001images/60.png\n",
      "saving class00001images/61.png\n",
      "saving class00001images/62.png\n",
      "saving class00001images/63.png\n",
      "saving class00001images/64.png\n",
      "saving class00001images/65.png\n",
      "saving class00001images/66.png\n",
      "saving class00001images/67.png\n",
      "saving class00001images/68.png\n",
      "saving class00001images/69.png\n",
      "saving class00001images/70.png\n",
      "saving class00001images/71.png\n",
      "saving class00001images/72.png\n",
      "saving class00001images/73.png\n",
      "saving class00001images/74.png\n",
      "saving class00001images/75.png\n",
      "saving class00001images/76.png\n",
      "saving class00001images/77.png\n",
      "saving class00001images/78.png\n",
      "saving class00001images/79.png\n",
      "saving class00001images/80.png\n",
      "saving class00001images/81.png\n",
      "saving class00001images/82.png\n",
      "saving class00001images/83.png\n",
      "saving class00001images/84.png\n",
      "saving class00001images/85.png\n",
      "saving class00001images/86.png\n",
      "saving class00001images/87.png\n",
      "saving class00001images/88.png\n",
      "saving class00001images/89.png\n",
      "saving class00001images/90.png\n",
      "saving class00001images/91.png\n",
      "saving class00001images/92.png\n",
      "saving class00001images/93.png\n",
      "saving class00001images/94.png\n",
      "saving class00001images/95.png\n",
      "saving class00001images/96.png\n",
      "saving class00001images/97.png\n",
      "saving class00001images/98.png\n",
      "saving class00001images/99.png\n",
      "saving class00001images/100.png\n",
      "saving class00001images/101.png\n",
      "saving class00001images/102.png\n",
      "saving class00001images/103.png\n",
      "saving class00001images/104.png\n",
      "saving class00001images/105.png\n",
      "saving class00001images/106.png\n",
      "saving class00001images/107.png\n",
      "saving class00001images/108.png\n",
      "saving class00001images/109.png\n",
      "saving class00001images/110.png\n",
      "saving class00001images/111.png\n",
      "saving class00001images/112.png\n",
      "saving class00001images/113.png\n",
      "saving class00001images/114.png\n",
      "saving class00001images/115.png\n",
      "saving class00001images/116.png\n",
      "saving class00001images/117.png\n",
      "saving class00001images/118.png\n",
      "saving class00001images/119.png\n",
      "saving class00001images/120.png\n",
      "saving class00001images/121.png\n",
      "saving class00001images/122.png\n",
      "saving class00001images/123.png\n",
      "saving class00001images/124.png\n",
      "saving class00001images/125.png\n",
      "saving class00001images/126.png\n",
      "saving class00001images/127.png\n",
      "saving class00001images/128.png\n",
      "saving class00001images/129.png\n",
      "saving class00001images/130.png\n",
      "saving class00001images/131.png\n",
      "saving class00001images/132.png\n",
      "saving class00001images/133.png\n",
      "saving class00001images/134.png\n",
      "saving class00001images/135.png\n",
      "saving class00001images/136.png\n",
      "saving class00001images/137.png\n",
      "saving class00001images/138.png\n",
      "saving class00001images/139.png\n",
      "saving class00001images/140.png\n",
      "saving class00001images/141.png\n",
      "saving class00001images/142.png\n",
      "saving class00001images/143.png\n",
      "saving class00001images/144.png\n",
      "saving class00001images/145.png\n",
      "saving class00001images/146.png\n",
      "saving class00001images/147.png\n",
      "saving class00001images/148.png\n",
      "saving class00001images/149.png\n",
      "saving class00001images/150.png\n",
      "saving class00001images/151.png\n",
      "saving class00001images/152.png\n",
      "saving class00001images/153.png\n",
      "saving class00001images/154.png\n",
      "saving class00001images/155.png\n",
      "saving class00001images/156.png\n",
      "saving class00001images/157.png\n",
      "saving class00001images/158.png\n",
      "saving class00001images/159.png\n",
      "saving class00001images/160.png\n",
      "saving class00001images/161.png\n",
      "saving class00001images/162.png\n",
      "saving class00001images/163.png\n",
      "saving class00001images/164.png\n",
      "saving class00001images/165.png\n",
      "saving class00001images/166.png\n",
      "saving class00001images/167.png\n",
      "saving class00001images/168.png\n",
      "saving class00001images/169.png\n",
      "saving class00001images/170.png\n",
      "saving class00001images/171.png\n",
      "saving class00001images/172.png\n",
      "saving class00001images/173.png\n",
      "saving class00001images/174.png\n",
      "saving class00001images/175.png\n",
      "saving class00001images/176.png\n",
      "saving class00001images/177.png\n",
      "saving class00001images/178.png\n",
      "saving class00001images/179.png\n",
      "saving class00001images/180.png\n",
      "saving class00001images/181.png\n",
      "saving class00001images/182.png\n",
      "saving class00001images/183.png\n",
      "saving class00001images/184.png\n",
      "saving class00001images/185.png\n",
      "saving class00001images/186.png\n",
      "saving class00001images/187.png\n",
      "saving class00001images/188.png\n",
      "saving class00001images/189.png\n",
      "saving class00001images/190.png\n",
      "saving class00001images/191.png\n",
      "saving class00001images/192.png\n",
      "saving class00001images/193.png\n",
      "saving class00001images/194.png\n",
      "saving class00001images/195.png\n",
      "saving class00001images/196.png\n",
      "saving class00001images/197.png\n",
      "saving class00001images/198.png\n",
      "saving class00001images/199.png\n",
      "saving class00001images/200.png\n",
      "saving class00001images/201.png\n",
      "saving class00001images/202.png\n",
      "saving class00001images/203.png\n",
      "saving class00001images/204.png\n",
      "saving class00001images/205.png\n",
      "saving class00001images/206.png\n",
      "saving class00001images/207.png\n",
      "saving class00001images/208.png\n",
      "saving class00001images/209.png\n",
      "saving class00001images/210.png\n",
      "saving class00001images/211.png\n",
      "saving class00001images/212.png\n",
      "saving class00001images/213.png\n",
      "saving class00001images/214.png\n",
      "saving class00001images/215.png\n",
      "saving class00001images/216.png\n",
      "saving class00001images/217.png\n",
      "saving class00001images/218.png\n",
      "saving class00001images/219.png\n",
      "saving class00001images/220.png\n",
      "saving class00001images/221.png\n",
      "saving class00001images/222.png\n",
      "saving class00001images/223.png\n",
      "saving class00001images/224.png\n",
      "saving class00001images/225.png\n",
      "saving class00001images/226.png\n",
      "saving class00001images/227.png\n",
      "saving class00001images/228.png\n",
      "saving class00001images/229.png\n",
      "saving class00001images/230.png\n",
      "saving class00001images/231.png\n",
      "saving class00001images/232.png\n",
      "saving class00001images/233.png\n",
      "saving class00001images/234.png\n",
      "saving class00001images/235.png\n",
      "saving class00001images/236.png\n",
      "saving class00001images/237.png\n",
      "saving class00001images/238.png\n",
      "saving class00001images/239.png\n",
      "saving class00001images/240.png\n",
      "saving class00001images/241.png\n",
      "saving class00001images/242.png\n",
      "saving class00001images/243.png\n",
      "saving class00001images/244.png\n",
      "saving class00001images/245.png\n",
      "saving class00001images/246.png\n",
      "saving class00001images/247.png\n",
      "saving class00001images/248.png\n",
      "saving class00001images/249.png\n",
      "saving class00001images/250.png\n",
      "saving class00001images/251.png\n",
      "saving class00001images/252.png\n",
      "saving class00001images/253.png\n",
      "saving class00001images/254.png\n",
      "saving class00001images/255.png\n",
      "saving class00001images/256.png\n",
      "saving class00001images/257.png\n",
      "saving class00001images/258.png\n",
      "saving class00001images/259.png\n",
      "saving class00001images/260.png\n",
      "saving class00001images/261.png\n",
      "saving class00001images/262.png\n",
      "saving class00001images/263.png\n",
      "saving class00001images/264.png\n",
      "saving class00001images/265.png\n",
      "saving class00001images/266.png\n",
      "saving class00001images/267.png\n",
      "saving class00001images/268.png\n",
      "saving class00001images/269.png\n",
      "saving class00001images/270.png\n",
      "saving class00001images/271.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving class00001images/272.png\n",
      "saving class00001images/273.png\n",
      "saving class00001images/274.png\n",
      "saving class00001images/275.png\n",
      "saving class00001images/276.png\n",
      "saving class00001images/277.png\n",
      "saving class00001images/278.png\n",
      "saving class00001images/279.png\n",
      "saving class00001images/280.png\n",
      "saving class00001images/281.png\n",
      "saving class00001images/282.png\n",
      "saving class00001images/283.png\n",
      "saving class00001images/284.png\n",
      "saving class00001images/285.png\n",
      "saving class00001images/286.png\n",
      "saving class00001images/287.png\n",
      "saving class00001images/288.png\n",
      "saving class00001images/289.png\n",
      "saving class00001images/290.png\n",
      "saving class00001images/291.png\n",
      "saving class00001images/292.png\n",
      "saving class00001images/293.png\n",
      "saving class00001images/294.png\n",
      "saving class00001images/295.png\n",
      "saving class00001images/296.png\n",
      "saving class00001images/297.png\n",
      "saving class00001images/298.png\n",
      "saving class00001images/299.png\n",
      "saving class00001images/300.png\n",
      "saving class00001images/301.png\n",
      "saving class00001images/302.png\n",
      "saving class00001images/303.png\n",
      "saving class00001images/304.png\n",
      "saving class00001images/305.png\n",
      "saving class00001images/306.png\n",
      "saving class00001images/307.png\n",
      "saving class00001images/308.png\n",
      "saving class00001images/309.png\n",
      "saving class00001images/310.png\n",
      "saving class00001images/311.png\n",
      "saving class00001images/312.png\n",
      "saving class00001images/313.png\n",
      "saving class00001images/314.png\n",
      "saving class00001images/315.png\n",
      "saving class00001images/316.png\n",
      "saving class00001images/317.png\n",
      "saving class00001images/318.png\n",
      "saving class00001images/319.png\n",
      "saving class00001images/320.png\n",
      "saving class00001images/321.png\n",
      "saving class00001images/322.png\n",
      "saving class00001images/323.png\n",
      "saving class00001images/324.png\n",
      "saving class00001images/325.png\n",
      "saving class00001images/326.png\n",
      "saving class00001images/327.png\n",
      "saving class00001images/328.png\n",
      "saving class00001images/329.png\n",
      "saving class00001images/330.png\n",
      "saving class00001images/331.png\n",
      "saving class00001images/332.png\n",
      "saving class00001images/333.png\n",
      "saving class00001images/334.png\n",
      "saving class00001images/335.png\n",
      "saving class00001images/336.png\n",
      "saving class00001images/337.png\n",
      "saving class00001images/338.png\n",
      "saving class00001images/339.png\n",
      "saving class00001images/340.png\n",
      "saving class00001images/341.png\n",
      "saving class00001images/342.png\n",
      "saving class00001images/343.png\n",
      "saving class00001images/344.png\n",
      "saving class00001images/345.png\n",
      "saving class00001images/346.png\n",
      "saving class00001images/347.png\n",
      "saving class00001images/348.png\n",
      "saving class00001images/349.png\n",
      "saving class00001images/350.png\n",
      "saving class00001images/351.png\n",
      "saving class00001images/352.png\n",
      "saving class00001images/353.png\n",
      "saving class00001images/354.png\n",
      "saving class00001images/355.png\n",
      "saving class00001images/356.png\n",
      "saving class00001images/357.png\n",
      "saving class00001images/358.png\n",
      "saving class00001images/359.png\n",
      "saving class00001images/360.png\n",
      "saving class00001images/361.png\n",
      "saving class00001images/362.png\n",
      "saving class00001images/363.png\n",
      "saving class00001images/364.png\n",
      "saving class00001images/365.png\n",
      "saving class00001images/366.png\n",
      "saving class00001images/367.png\n",
      "saving class00001images/368.png\n",
      "saving class00001images/369.png\n",
      "saving class00001images/370.png\n",
      "saving class00001images/371.png\n",
      "saving class00001images/372.png\n",
      "saving class00001images/373.png\n",
      "saving class00001images/374.png\n",
      "saving class00001images/375.png\n",
      "saving class00001images/376.png\n",
      "saving class00001images/377.png\n",
      "saving class00001images/378.png\n",
      "saving class00001images/379.png\n",
      "saving class00001images/380.png\n",
      "saving class00001images/381.png\n",
      "saving class00001images/382.png\n",
      "saving class00001images/383.png\n",
      "saving class00001images/384.png\n",
      "saving class00001images/385.png\n",
      "saving class00001images/386.png\n",
      "saving class00001images/387.png\n",
      "saving class00001images/388.png\n",
      "saving class00001images/389.png\n",
      "saving class00001images/390.png\n",
      "saving class00001images/391.png\n",
      "saving class00001images/392.png\n",
      "saving class00001images/393.png\n",
      "saving class00001images/394.png\n",
      "saving class00001images/395.png\n",
      "saving class00001images/396.png\n",
      "saving class00001images/397.png\n",
      "saving class00001images/398.png\n",
      "saving class00001images/399.png\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(alllines), 8):\n",
    "    sample_size = batch_size\n",
    "    sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, z_dim)).astype(np.float32)\n",
    "        # sample_seed = np.random.uniform(low=-1, high=1, size=(sample_size, z_dim)).astype(np.float32)]\n",
    "    n = int(sample_size / ni)\n",
    "    sample_sentence = [alllines[i]] * n + \\\n",
    "                    [alllines[i+1]] * n + \\\n",
    "                    [alllines[i+2]] * n + \\\n",
    "                    [alllines[i+3]] * n + \\\n",
    "                    [alllines[i+4]] * n + \\\n",
    "                    [alllines[i+5]] * n + \\\n",
    "                    [alllines[i+6]] * n + \\\n",
    "                    [alllines[i+7]] * n \n",
    "    for j, sentence in enumerate(sample_sentence):\n",
    "#         print(\"seed: %s\" % sentence)\n",
    "        sentence = preprocess_caption(sentence)\n",
    "        sample_sentence[j] = [vocab.word_to_id(word) for word in nltk.tokenize.word_tokenize(sentence)] + [vocab.end_id]    # add END_ID\n",
    "        # sample_sentence[i] = [vocab.word_to_id(word) for word in sentence]\n",
    "        # print(sample_sentence[i])\n",
    "    sample_sentence = tl.prepro.pad_sequences(sample_sentence, padding='post')   \n",
    "    img_gen, rnn_out = sess.run([net_g.outputs, net_rnn.outputs], feed_dict={\n",
    "                                    t_real_caption : sample_sentence,\n",
    "                                    t_z : sample_seed})\n",
    "    for index in range(0, 64, 8):\n",
    "        imagetochoose = index + randrange(8)\n",
    "        name = \"class00001images/\" + str(int(index / 8 + i)) + \".png\"\n",
    "        print(\"saving\", name)\n",
    "        scipy.misc.imsave(name, img_gen[imagetochoose])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alllines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_gen, rnn_out = sess.run([net_g.outputs, net_rnn.outputs], feed_dict={\n",
    "                                    t_real_caption : sample_sentence,\n",
    "                                    t_z : sample_seed})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_images(img_gen, [ni, ni], 'samples/step1_gan-cls/pinkflowerdottest.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gen[63].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cseuser\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# code to save an image\n",
    "scipy.misc.imsave(\"class00001images\\1.png\", img_gen[63])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Will actually train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epoch = 10 # 600\n",
    "print_freq = 1\n",
    "n_batch_epoch = int(n_images_train / batch_size)\n",
    "# exit()\n",
    "for epoch in range(0, n_epoch+1):\n",
    "    start_time = time.time()\n",
    "\n",
    "    if epoch !=0 and (epoch % decay_every == 0):\n",
    "        new_lr_decay = lr_decay ** (epoch // decay_every)\n",
    "        sess.run(tf.assign(lr_v, lr * new_lr_decay))\n",
    "        log = \" ** new learning rate: %f\" % (lr * new_lr_decay)\n",
    "        print(log)\n",
    "        # logging.debug(log)\n",
    "    elif epoch == 0:\n",
    "        log = \" ** init lr: %f  decay_every_epoch: %d, lr_decay: %f\" % (lr, decay_every, lr_decay)\n",
    "        print(log)\n",
    "\n",
    "    for step in range(n_batch_epoch):\n",
    "        step_time = time.time()\n",
    "        ## get matched text\n",
    "        idexs = get_random_int(min=0, max=n_captions_train-1, number=batch_size)\n",
    "        b_real_caption = captions_ids_train[idexs]\n",
    "        b_real_caption = tl.prepro.pad_sequences(b_real_caption, padding='post')\n",
    "        ## get real image\n",
    "        b_real_images = images_train[np.floor(np.asarray(idexs).astype('float')/n_captions_per_image).astype('int')]\n",
    "        # save_images(b_real_images, [ni, ni], 'samples/step1_gan-cls/train_00.png')\n",
    "        ## get wrong caption\n",
    "        idexs = get_random_int(min=0, max=n_captions_train-1, number=batch_size)\n",
    "        b_wrong_caption = captions_ids_train[idexs]\n",
    "        b_wrong_caption = tl.prepro.pad_sequences(b_wrong_caption, padding='post')\n",
    "        ## get wrong image\n",
    "        idexs2 = get_random_int(min=0, max=n_images_train-1, number=batch_size)\n",
    "        b_wrong_images = images_train[idexs2]\n",
    "        ## get noise\n",
    "        b_z = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, z_dim)).astype(np.float32)\n",
    "            # b_z = np.random.uniform(low=-1, high=1, size=[batch_size, z_dim]).astype(np.float32)\n",
    "\n",
    "        b_real_images = threading_data(b_real_images, prepro_img, mode='train')   # [0, 255] --> [-1, 1] + augmentation\n",
    "        b_wrong_images = threading_data(b_wrong_images, prepro_img, mode='train')\n",
    "        ## updates text-to-image mapping\n",
    "        if epoch < 50:\n",
    "            errRNN, _ = sess.run([rnn_loss, rnn_optim], feed_dict={\n",
    "                                            t_real_image : b_real_images,\n",
    "                                            t_wrong_image : b_wrong_images,\n",
    "                                            t_real_caption : b_real_caption,\n",
    "                                            t_wrong_caption : b_wrong_caption})\n",
    "        else:\n",
    "            errRNN = 0\n",
    "\n",
    "        ## updates D\n",
    "        errD, _ = sess.run([d_loss, d_optim], feed_dict={\n",
    "                        t_real_image : b_real_images,\n",
    "                        # t_wrong_image : b_wrong_images,\n",
    "                        t_wrong_caption : b_wrong_caption,\n",
    "                        t_real_caption : b_real_caption,\n",
    "                        t_z : b_z})\n",
    "        ## updates G\n",
    "        errG, _ = sess.run([g_loss, g_optim], feed_dict={\n",
    "                        t_real_caption : b_real_caption,\n",
    "                        t_z : b_z})\n",
    "\n",
    "        print(\"Epoch: [%2d/%2d] [%4d/%4d] time: %4.4fs, d_loss: %.8f, g_loss: %.8f, rnn_loss: %.8f\" \\\n",
    "                    % (epoch, n_epoch, step, n_batch_epoch, time.time() - step_time, errD, errG, errRNN))\n",
    "\n",
    "    if (epoch + 1) % print_freq == 0:\n",
    "        print(\" ** Epoch %d took %fs\" % (epoch, time.time()-start_time))\n",
    "        img_gen, rnn_out = sess.run([net_g.outputs, net_rnn.outputs], feed_dict={\n",
    "                                    t_real_caption : sample_sentence,\n",
    "                                    t_z : sample_seed})\n",
    "\n",
    "        # img_gen = threading_data(img_gen, prepro_img, mode='rescale')\n",
    "        save_images(img_gen, [ni, ni], 'samples/step1_gan-cls/train_{:02d}.png'.format(epoch))\n",
    "\n",
    "    ## save model\n",
    "    if (epoch != 0) and (epoch % 2) == 0:\n",
    "        tl.files.save_npz(net_cnn.all_params, name=net_cnn_name, sess=sess)\n",
    "        tl.files.save_npz(net_rnn.all_params, name=net_rnn_name, sess=sess)\n",
    "        tl.files.save_npz(net_g.all_params, name=net_g_name, sess=sess)\n",
    "        tl.files.save_npz(net_d.all_params, name=net_d_name, sess=sess)\n",
    "        print(\"[*] Save checkpoints SUCCESS!\")\n",
    "\n",
    "    if (epoch != 0) and (epoch % 10) == 0:\n",
    "        tl.files.save_npz(net_cnn.all_params, name=net_cnn_name+str(epoch), sess=sess)\n",
    "        tl.files.save_npz(net_rnn.all_params, name=net_rnn_name+str(epoch), sess=sess)\n",
    "        tl.files.save_npz(net_g.all_params, name=net_g_name+str(epoch), sess=sess)\n",
    "        tl.files.save_npz(net_d.all_params, name=net_d_name+str(epoch), sess=sess)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
